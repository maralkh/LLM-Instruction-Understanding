{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 9: System Prompt Length and Position Effects\n",
    "\n",
    "**Goal:** Test how system prompt LENGTH and component ORDER affect behavior.\n",
    "\n",
    "**Setup:**\n",
    "- Fixed test prompts\n",
    "- Same content at different lengths (repetition, verbosity)\n",
    "- Same components in different orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.metrics import DistributionMetrics\n",
    "from src.visualization import set_style\n",
    "from src.test_configs import ALL_TEST_PROMPTS, build_chat_prompt\n",
    "\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Length Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same instruction at different lengths\n",
    "BASE_INSTRUCTION = \"Be helpful and accurate.\"\n",
    "\n",
    "LENGTH_VARIANTS = {\n",
    "    \"minimal\": \"Be helpful.\",\n",
    "    \"short\": \"Be helpful and accurate.\",\n",
    "    \"medium\": \"You are a helpful assistant. Please provide accurate and useful information.\",\n",
    "    \"long\": \"\"\"You are a helpful AI assistant. Your goal is to provide accurate, \n",
    "useful, and well-reasoned responses. Always strive to be helpful while \n",
    "ensuring the information you provide is correct and reliable.\"\"\",\n",
    "    \"very_long\": \"\"\"You are a helpful AI assistant created to assist users with \n",
    "their questions and tasks. Your primary goal is to provide accurate, useful, \n",
    "and well-reasoned responses to any query. Always strive to be helpful while \n",
    "ensuring the information you provide is correct, reliable, and up-to-date. \n",
    "Take care to understand what the user is asking and respond appropriately.\n",
    "If you're uncertain about something, say so clearly.\"\"\",\n",
    "    \"repeated\": \"Be helpful. Be accurate. Be helpful. Be accurate. Be helpful.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUBSET = ALL_TEST_PROMPTS[:10]\n",
    "\n",
    "length_results = []\n",
    "for variant_name, variant_text in tqdm(LENGTH_VARIANTS.items()):\n",
    "    for test in TEST_SUBSET:\n",
    "        prompt = build_chat_prompt(variant_text, test[\"prompt\"], model.tokenizer)\n",
    "        dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "        \n",
    "        length_results.append({\n",
    "            \"variant\": variant_name,\n",
    "            \"length_chars\": len(variant_text),\n",
    "            \"length_words\": len(variant_text.split()),\n",
    "            \"test_id\": test[\"id\"],\n",
    "            \"entropy\": dist[\"entropy\"],\n",
    "            \"top_prob\": dist[\"top_probs\"][0],\n",
    "            \"full_probs\": dist[\"full_probs\"]\n",
    "        })\n",
    "\n",
    "length_df = pd.DataFrame(length_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze length effects\n",
    "length_summary = length_df.groupby(['variant', 'length_words']).agg({\n",
    "    'entropy': 'mean',\n",
    "    'top_prob': 'mean'\n",
    "}).reset_index().sort_values('length_words')\n",
    "\n",
    "print(\"=== Length Effects ===\")\n",
    "print(length_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Entropy vs length\n",
    "ax = axes[0]\n",
    "ax.scatter(length_summary['length_words'], length_summary['entropy'], s=100, alpha=0.7)\n",
    "for _, row in length_summary.iterrows():\n",
    "    ax.annotate(row['variant'], (row['length_words'], row['entropy']), \n",
    "                textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "ax.set_xlabel('System Prompt Length (words)')\n",
    "ax.set_ylabel('Mean Entropy')\n",
    "ax.set_title('Does Longer System Prompt = Different Behavior?')\n",
    "\n",
    "# Confidence vs length\n",
    "ax = axes[1]\n",
    "ax.scatter(length_summary['length_words'], length_summary['top_prob'], s=100, alpha=0.7)\n",
    "ax.set_xlabel('System Prompt Length (words)')\n",
    "ax.set_ylabel('Mean Top Token Probability')\n",
    "ax.set_title('Confidence vs System Prompt Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp9_length_effects.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Order Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components in different orders\n",
    "COMPONENTS = [\n",
    "    \"Be helpful.\",\n",
    "    \"Be accurate.\", \n",
    "    \"Think step by step.\",\n",
    "    \"Be concise.\"\n",
    "]\n",
    "\n",
    "# Generate permutations\n",
    "from itertools import permutations\n",
    "\n",
    "ORDER_VARIANTS = {}\n",
    "all_perms = list(permutations(range(len(COMPONENTS))))\n",
    "# Sample 6 permutations\n",
    "random.seed(42)\n",
    "sampled_perms = random.sample(all_perms, min(6, len(all_perms)))\n",
    "\n",
    "for i, perm in enumerate(sampled_perms):\n",
    "    ordered = \" \".join([COMPONENTS[j] for j in perm])\n",
    "    ORDER_VARIANTS[f\"order_{i}\"] = {\n",
    "        \"text\": ordered,\n",
    "        \"order\": perm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_results = []\n",
    "for variant_name, variant_info in tqdm(ORDER_VARIANTS.items()):\n",
    "    for test in TEST_SUBSET:\n",
    "        prompt = build_chat_prompt(variant_info[\"text\"], test[\"prompt\"], model.tokenizer)\n",
    "        dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "        \n",
    "        order_results.append({\n",
    "            \"variant\": variant_name,\n",
    "            \"order\": str(variant_info[\"order\"]),\n",
    "            \"first_component\": COMPONENTS[variant_info[\"order\"][0]],\n",
    "            \"last_component\": COMPONENTS[variant_info[\"order\"][-1]],\n",
    "            \"test_id\": test[\"id\"],\n",
    "            \"entropy\": dist[\"entropy\"],\n",
    "            \"full_probs\": dist[\"full_probs\"]\n",
    "        })\n",
    "\n",
    "order_df = pd.DataFrame(order_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze order effects\n",
    "order_summary = order_df.groupby('variant').agg({\n",
    "    'entropy': ['mean', 'std'],\n",
    "    'first_component': 'first',\n",
    "    'last_component': 'first'\n",
    "}).round(4)\n",
    "order_summary.columns = ['entropy_mean', 'entropy_std', 'first', 'last']\n",
    "\n",
    "print(\"=== Order Effects ===\")\n",
    "print(order_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pairwise JS between different orders\n",
    "baseline_order = list(ORDER_VARIANTS.keys())[0]\n",
    "baseline_by_test = {r[\"test_id\"]: r[\"full_probs\"] \n",
    "                    for r in order_results if r[\"variant\"] == baseline_order}\n",
    "\n",
    "order_divergences = []\n",
    "for variant in ORDER_VARIANTS.keys():\n",
    "    if variant == baseline_order:\n",
    "        continue\n",
    "    \n",
    "    js_values = []\n",
    "    for _, row in order_df[order_df['variant'] == variant].iterrows():\n",
    "        js = DistributionMetrics.jensen_shannon(\n",
    "            baseline_by_test[row['test_id']], row['full_probs']\n",
    "        )\n",
    "        js_values.append(js)\n",
    "    \n",
    "    order_divergences.append({\n",
    "        \"variant\": variant,\n",
    "        \"js_from_baseline\": np.mean(js_values)\n",
    "    })\n",
    "\n",
    "divergence_df = pd.DataFrame(order_divergences)\n",
    "print(f\"\\nMean JS divergence between orderings: {divergence_df['js_from_baseline'].mean():.4f}\")\n",
    "print(f\"Max JS divergence: {divergence_df['js_from_baseline'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does position matter? Check if first/last component affects outcome\n",
    "by_first = order_df.groupby('first_component')['entropy'].mean()\n",
    "by_last = order_df.groupby('last_component')['entropy'].mean()\n",
    "\n",
    "print(\"\\n=== Position Effects ===\")\n",
    "print(\"\\nBy FIRST component:\")\n",
    "print(by_first.sort_values())\n",
    "\n",
    "print(\"\\nBy LAST component:\")\n",
    "print(by_last.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 9 SUMMARY: Length and Order Effects\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Length\n",
    "length_corr = length_summary['length_words'].corr(length_summary['entropy'])\n",
    "print(f\"\\n1. Length-Entropy Correlation: {length_corr:.3f}\")\n",
    "if abs(length_corr) > 0.3:\n",
    "    print(\"   → System prompt length DOES affect behavior\")\n",
    "else:\n",
    "    print(\"   → System prompt length has minimal effect\")\n",
    "\n",
    "# Order\n",
    "order_variance = order_summary['entropy_mean'].var()\n",
    "print(f\"\\n2. Order Variance in Entropy: {order_variance:.6f}\")\n",
    "if order_variance > 0.01:\n",
    "    print(\"   → Component order MATTERS\")\n",
    "else:\n",
    "    print(\"   → Component order has minimal effect\")\n",
    "\n",
    "# Position\n",
    "first_range = by_first.max() - by_first.min()\n",
    "last_range = by_last.max() - by_last.min()\n",
    "print(f\"\\n3. First component effect: {first_range:.4f}\")\n",
    "print(f\"   Last component effect: {last_range:.4f}\")\n",
    "if last_range > first_range:\n",
    "    print(\"   → LAST component has more influence (recency bias)\")\n",
    "else:\n",
    "    print(\"   → FIRST component has more influence (primacy bias)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../results/exp9_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"length_effects\": length_summary.to_dict('records'),\n",
    "        \"order_effects\": order_summary.to_dict(),\n",
    "        \"position_effects\": {\n",
    "            \"by_first\": by_first.to_dict(),\n",
    "            \"by_last\": by_last.to_dict()\n",
    "        }\n",
    "    }, f, indent=2, default=float)\n",
    "print(\"Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
