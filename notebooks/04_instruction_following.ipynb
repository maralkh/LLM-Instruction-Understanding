{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: Instruction Following Consistency\n",
    "\n",
    "**Goal:** Measure how well models follow explicit instructions in system prompts.\n",
    "\n",
    "**Setup:**\n",
    "- Fixed test prompts\n",
    "- System prompts with specific behavioral instructions\n",
    "- Measure compliance rate for each instruction type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup path for imports\nimport sys\nimport os\n\n# Handle both local and Colab environments\nif 'google.colab' in sys.modules:\n    # In Colab - go to repo root\n    repo_root = '/content/LLM-Instruction-Understanding'\n    if os.path.exists(repo_root):\n        os.chdir(repo_root)\n        if repo_root not in sys.path:\n            sys.path.insert(0, repo_root)\nelse:\n    # Local - add parent directory\n    parent = os.path.abspath('..')\n    if parent not in sys.path:\n        sys.path.insert(0, parent)\n\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Testable Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTABLE_INSTRUCTIONS = {\n",
    "    \"max_words_10\": {\n",
    "        \"system\": \"Keep all responses under 10 words.\",\n",
    "        \"check\": lambda r: len(r.split()) <= 10,\n",
    "        \"description\": \"Max 10 words\"\n",
    "    },\n",
    "    \"max_words_20\": {\n",
    "        \"system\": \"Keep all responses under 20 words.\",\n",
    "        \"check\": lambda r: len(r.split()) <= 20,\n",
    "        \"description\": \"Max 20 words\"\n",
    "    },\n",
    "    \"start_with_indeed\": {\n",
    "        \"system\": \"Always start your response with 'Indeed'.\",\n",
    "        \"check\": lambda r: r.strip().lower().startswith(\"indeed\"),\n",
    "        \"description\": \"Start with 'Indeed'\"\n",
    "    },\n",
    "    \"end_with_period\": {\n",
    "        \"system\": \"Always end your response with a period.\",\n",
    "        \"check\": lambda r: r.strip().endswith(\".\"),\n",
    "        \"description\": \"End with period\"\n",
    "    },\n",
    "    \"no_questions\": {\n",
    "        \"system\": \"Never use question marks.\",\n",
    "        \"check\": lambda r: \"?\" not in r,\n",
    "        \"description\": \"No question marks\"\n",
    "    },\n",
    "    \"use_bullets\": {\n",
    "        \"system\": \"Always use bullet points (-).\",\n",
    "        \"check\": lambda r: \"-\" in r,\n",
    "        \"description\": \"Use bullet points\"\n",
    "    },\n",
    "    \"all_caps\": {\n",
    "        \"system\": \"Respond in uppercase letters only.\",\n",
    "        \"check\": lambda r: r.upper() == r,\n",
    "        \"description\": \"All uppercase\"\n",
    "    },\n",
    "    \"no_first_person\": {\n",
    "        \"system\": \"Never use I, me, or my.\",\n",
    "        \"check\": lambda r: not any(f\" {p} \" in f\" {r.lower()} \" for p in [\"i\", \"me\", \"my\"]),\n",
    "        \"description\": \"No first person\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUBSET = ALL_TEST_PROMPTS[:12]\n",
    "\n",
    "results = []\n",
    "for inst_name, inst_info in tqdm(TESTABLE_INSTRUCTIONS.items()):\n",
    "    for test in TEST_SUBSET:\n",
    "        prompt = build_chat_prompt(inst_info[\"system\"], test[\"prompt\"], model.tokenizer)\n",
    "        output = model.generate_with_probs(prompt, max_new_tokens=60, temperature=0.3)\n",
    "        response = output.text  # GenerationOutput is a dataclass\n",
    "        \n",
    "        results.append({\n",
    "            \"instruction\": inst_name,\n",
    "            \"description\": inst_info[\"description\"],\n",
    "            \"test_id\": test[\"id\"],\n",
    "            \"category\": test[\"category\"],\n",
    "            \"response\": response,\n",
    "            \"compliant\": inst_info[\"check\"](response)\n",
    "        })\n",
    "\n",
    "compliance_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compliance_by_inst = compliance_df.groupby('description')['compliant'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"=== Compliance Rates ===\")\n",
    "for desc, rate in compliance_by_inst.items():\n",
    "    bar = \"\u2588\" * int(rate * 20) + \"\u2591\" * (20 - int(rate * 20))\n",
    "    print(f\"{desc:25s} {bar} {rate*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = plt.cm.RdYlGn(compliance_by_inst.values)\n",
    "ax.barh(range(len(compliance_by_inst)), compliance_by_inst.values * 100, color=colors)\n",
    "ax.set_yticks(range(len(compliance_by_inst)))\n",
    "ax.set_yticklabels(compliance_by_inst.index)\n",
    "ax.set_xlabel('Compliance Rate (%)')\n",
    "ax.set_title('Instruction Following Rates')\n",
    "ax.axvline(x=50, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp4_compliance.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap by category\n",
    "pivot = compliance_df.pivot_table(values='compliant', index='description', columns='category', aggfunc='mean')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(pivot, annot=True, fmt='.0%', cmap='RdYlGn', ax=ax)\n",
    "ax.set_title('Compliance by Instruction \u00d7 Category')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp4_heatmap.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Overall compliance: {compliance_df['compliant'].mean()*100:.1f}%\")\n",
    "print(f\"Best: {compliance_by_inst.idxmax()} ({compliance_by_inst.max()*100:.0f}%)\")\n",
    "print(f\"Worst: {compliance_by_inst.idxmin()} ({compliance_by_inst.min()*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../results/exp4_results.json', 'w') as f:\n",
    "    json.dump({\"compliance_rates\": compliance_by_inst.to_dict()}, f, indent=2)\n",
    "print(\"Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}