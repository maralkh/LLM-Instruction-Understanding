{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Base vs Instruction-Tuned Model Comparison\n",
    "\n",
    "**Goal:** Understand how instruction tuning changes prompt sensitivity.\n",
    "\n",
    "**Key Questions:**\n",
    "- Do instruction-tuned models respond differently to prompt variations?\n",
    "- Which prompt strategies become more/less effective after instruction tuning?\n",
    "- How does the \"assistant\" role affect model behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.model_utils import load_model, ModelConfig\n",
    "from src.prompt_utils import PromptVariantGenerator, ASSISTANT_PREFIXES\n",
    "from src.metrics import DistributionMetrics, ExperimentResults, ComparisonMetrics\n",
    "from src.visualization import set_style, plot_model_comparison, plot_distribution_comparison\n",
    "\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Both Models\n",
    "\n",
    "We'll compare:\n",
    "- **Base model**: TinyLlama-1.1B (pre-trained only)\n",
    "- **Instruction-tuned**: TinyLlama-1.1B-Chat (fine-tuned for chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models - they'll share GPU memory efficiently if using same architecture\n",
    "MODELS = {\n",
    "    \"base\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
    "    \"chat\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "}\n",
    "\n",
    "# Load base model first\n",
    "print(\"Loading base model...\")\n",
    "model_base = load_model(MODELS[\"base\"])\n",
    "\n",
    "print(\"\\nLoading chat model...\")\n",
    "model_chat = load_model(MODELS[\"chat\"])\n",
    "\n",
    "models = {\"base\": model_base, \"chat\": model_chat}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CASES = {\n",
    "    \"factual\": {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"expected\": \"Paris\"\n",
    "    },\n",
    "    \"reasoning\": {\n",
    "        \"question\": \"If John has 5 apples and gives 2 to Mary, how many does John have left?\",\n",
    "        \"expected\": \"3\"\n",
    "    },\n",
    "    \"instruction_following\": {\n",
    "        \"question\": \"List three primary colors.\",\n",
    "        \"expected\": \"red\"  # Check if it starts listing\n",
    "    },\n",
    "    \"classification\": {\n",
    "        \"question\": \"Is the following positive or negative: 'I love this product!'\",\n",
    "        \"expected\": \"positive\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Prompt Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_on_variants(models, question, expected, dimensions):\n",
    "    \"\"\"\n",
    "    Compare how different models respond to the same prompt variants.\n",
    "    \"\"\"\n",
    "    variants = PromptVariantGenerator.create_variants(question, dimensions=dimensions)\n",
    "    \n",
    "    results = {model_name: [] for model_name in models}\n",
    "    \n",
    "    for variant in tqdm(variants, desc=\"Testing variants\"):\n",
    "        prompt = variant[\"prompt\"]\n",
    "        config = variant[\"config\"]\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            # Get distribution\n",
    "            dist = model.get_next_token_distribution(prompt)\n",
    "            \n",
    "            # Get probability of expected completion\n",
    "            seq_probs = model.get_sequence_log_probs(prompt, \" \" + expected)\n",
    "            \n",
    "            results[model_name].append({\n",
    "                \"config\": config,\n",
    "                \"prompt\": prompt,\n",
    "                \"entropy\": dist[\"entropy\"],\n",
    "                \"target_log_prob\": seq_probs[\"total_log_prob\"],\n",
    "                \"top_5\": dist[\"top_tokens\"][:5]\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "all_comparisons = {}\n",
    "\n",
    "for task_name, task_data in TEST_CASES.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task: {task_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    comparison = compare_models_on_variants(\n",
    "        models,\n",
    "        task_data[\"question\"],\n",
    "        task_data[\"expected\"],\n",
    "        dimensions=['specificity', 'format']\n",
    "    )\n",
    "    \n",
    "    all_comparisons[task_name] = comparison\n",
    "    \n",
    "    # Quick summary\n",
    "    for model_name in models:\n",
    "        log_probs = [r[\"target_log_prob\"] for r in comparison[model_name]]\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Target log-prob: mean={np.mean(log_probs):.3f}, std={np.std(log_probs):.3f}\")\n",
    "        print(f\"  Range: [{np.min(log_probs):.3f}, {np.max(log_probs):.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Prompt Sensitivity Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sensitivity(results):\n",
    "    \"\"\"\n",
    "    Calculate how sensitive a model is to prompt variations.\n",
    "    Higher variance = more sensitive\n",
    "    \"\"\"\n",
    "    log_probs = [r[\"target_log_prob\"] for r in results]\n",
    "    entropies = [r[\"entropy\"] for r in results]\n",
    "    \n",
    "    return {\n",
    "        \"log_prob_variance\": np.var(log_probs),\n",
    "        \"log_prob_range\": np.max(log_probs) - np.min(log_probs),\n",
    "        \"entropy_variance\": np.var(entropies),\n",
    "        \"mean_entropy\": np.mean(entropies)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sensitivity across models and tasks\n",
    "sensitivity_data = []\n",
    "\n",
    "for task_name, comparison in all_comparisons.items():\n",
    "    for model_name, results in comparison.items():\n",
    "        sens = calculate_sensitivity(results)\n",
    "        sensitivity_data.append({\n",
    "            \"task\": task_name,\n",
    "            \"model\": model_name,\n",
    "            **sens\n",
    "        })\n",
    "\n",
    "sens_df = pd.DataFrame(sensitivity_data)\n",
    "print(\"\\n=== Prompt Sensitivity Comparison ===\")\n",
    "print(sens_df.pivot(index='task', columns='model', values='log_prob_variance').round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensitivity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Log-prob variance by model\n",
    "pivot_var = sens_df.pivot(index='task', columns='model', values='log_prob_variance')\n",
    "pivot_var.plot(kind='bar', ax=axes[0], color=['steelblue', 'coral'])\n",
    "axes[0].set_title('Prompt Sensitivity (Log-Prob Variance)')\n",
    "axes[0].set_ylabel('Variance')\n",
    "axes[0].legend(title='Model')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Mean entropy by model\n",
    "pivot_ent = sens_df.pivot(index='task', columns='model', values='mean_entropy')\n",
    "pivot_ent.plot(kind='bar', ax=axes[1], color=['steelblue', 'coral'])\n",
    "axes[1].set_title('Mean Output Entropy')\n",
    "axes[1].set_ylabel('Entropy (nats)')\n",
    "axes[1].legend(title='Model')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp3_sensitivity_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assistant Prefix Experiment\n",
    "\n",
    "Test how different assistant response prefixes affect model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assistant prefixes\n",
    "ASSISTANT_TEST_PREFIXES = {\n",
    "    \"none\": \"\",\n",
    "    \"sure\": \"Sure! \",\n",
    "    \"certainly\": \"Certainly. \",\n",
    "    \"lets_think\": \"Let me think about this. \",\n",
    "    \"step_by_step\": \"I'll solve this step by step. \",\n",
    "    \"great_question\": \"Great question! \",\n",
    "    \"hmm\": \"Hmm, \",\n",
    "    \"well\": \"Well, \"\n",
    "}\n",
    "\n",
    "def test_assistant_prefixes(model, question, expected, prefixes):\n",
    "    \"\"\"Test how assistant prefixes affect completion probability.\"\"\"\n",
    "    base_prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    \n",
    "    results = []\n",
    "    for prefix_name, prefix in prefixes.items():\n",
    "        prompt = base_prompt + prefix\n",
    "        \n",
    "        dist = model.get_next_token_distribution(prompt)\n",
    "        seq_probs = model.get_sequence_log_probs(prompt, expected)\n",
    "        \n",
    "        results.append({\n",
    "            \"prefix\": prefix_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"target_log_prob\": seq_probs[\"total_log_prob\"],\n",
    "            \"entropy\": dist[\"entropy\"],\n",
    "            \"top_5\": dist[\"top_tokens\"][:5]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run assistant prefix test on both models\n",
    "prefix_results = {}\n",
    "\n",
    "test_question = \"What is 15 + 27?\"\n",
    "test_expected = \"42\"\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTesting {model_name} model...\")\n",
    "    results = test_assistant_prefixes(model, test_question, test_expected, ASSISTANT_TEST_PREFIXES)\n",
    "    prefix_results[model_name] = results\n",
    "    \n",
    "    # Show results\n",
    "    for r in sorted(results, key=lambda x: x[\"target_log_prob\"], reverse=True):\n",
    "        print(f\"  {r['prefix']:20s}: log_prob={r['target_log_prob']:.3f}, entropy={r['entropy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize assistant prefix effects\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for idx, (model_name, results) in enumerate(prefix_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    sorted_results = sorted(results, key=lambda x: x[\"target_log_prob\"], reverse=True)\n",
    "    prefixes = [r[\"prefix\"] for r in sorted_results]\n",
    "    log_probs = [r[\"target_log_prob\"] for r in sorted_results]\n",
    "    \n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(prefixes)))\n",
    "    ax.barh(range(len(prefixes)), log_probs, color=colors)\n",
    "    ax.set_yticks(range(len(prefixes)))\n",
    "    ax.set_yticklabels(prefixes)\n",
    "    ax.set_xlabel('Target Log Probability')\n",
    "    ax.set_title(f'Assistant Prefix Effect: {model_name}')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp3_assistant_prefix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chat Template Effect (Instruction-Tuned Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with proper chat template vs raw prompt\n",
    "def test_chat_template(model, tokenizer_name, question, expected):\n",
    "    \"\"\"Compare raw prompt vs chat template.\"\"\"\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    # Raw prompt\n",
    "    raw_prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    \n",
    "    # Chat template (if available)\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        try:\n",
    "            chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        except:\n",
    "            chat_prompt = raw_prompt\n",
    "    else:\n",
    "        chat_prompt = raw_prompt\n",
    "    \n",
    "    # Evaluate both\n",
    "    raw_dist = model.get_next_token_distribution(raw_prompt)\n",
    "    raw_probs = model.get_sequence_log_probs(raw_prompt, \" \" + expected)\n",
    "    \n",
    "    chat_dist = model.get_next_token_distribution(chat_prompt)\n",
    "    chat_probs = model.get_sequence_log_probs(chat_prompt, \" \" + expected)\n",
    "    \n",
    "    return {\n",
    "        \"raw\": {\n",
    "            \"prompt\": raw_prompt,\n",
    "            \"log_prob\": raw_probs[\"total_log_prob\"],\n",
    "            \"entropy\": raw_dist[\"entropy\"],\n",
    "            \"top_5\": raw_dist[\"top_tokens\"][:5]\n",
    "        },\n",
    "        \"chat\": {\n",
    "            \"prompt\": chat_prompt,\n",
    "            \"log_prob\": chat_probs[\"total_log_prob\"],\n",
    "            \"entropy\": chat_dist[\"entropy\"],\n",
    "            \"top_5\": chat_dist[\"top_tokens\"][:5]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chat template on chat model\n",
    "print(\"Testing chat template effect...\")\n",
    "\n",
    "template_results = {}\n",
    "for task_name, task_data in TEST_CASES.items():\n",
    "    result = test_chat_template(\n",
    "        model_chat, \n",
    "        MODELS[\"chat\"],\n",
    "        task_data[\"question\"],\n",
    "        task_data[\"expected\"]\n",
    "    )\n",
    "    template_results[task_name] = result\n",
    "    \n",
    "    print(f\"\\n{task_name}:\")\n",
    "    print(f\"  Raw prompt log-prob: {result['raw']['log_prob']:.3f}\")\n",
    "    print(f\"  Chat template log-prob: {result['chat']['log_prob']:.3f}\")\n",
    "    print(f\"  Improvement: {result['chat']['log_prob'] - result['raw']['log_prob']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 3 SUMMARY: Base vs Instruction-Tuned\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Aggregate findings\n",
    "print(\"\\n1. Prompt Sensitivity:\")\n",
    "base_vars = [s[\"log_prob_variance\"] for s in sensitivity_data if s[\"model\"] == \"base\"]\n",
    "chat_vars = [s[\"log_prob_variance\"] for s in sensitivity_data if s[\"model\"] == \"chat\"]\n",
    "print(f\"   - Base model avg variance: {np.mean(base_vars):.4f}\")\n",
    "print(f\"   - Chat model avg variance: {np.mean(chat_vars):.4f}\")\n",
    "print(f\"   - Chat model is {'more' if np.mean(chat_vars) > np.mean(base_vars) else 'less'} sensitive to prompts\")\n",
    "\n",
    "print(\"\\n2. Output Entropy:\")\n",
    "base_ent = [s[\"mean_entropy\"] for s in sensitivity_data if s[\"model\"] == \"base\"]\n",
    "chat_ent = [s[\"mean_entropy\"] for s in sensitivity_data if s[\"model\"] == \"chat\"]\n",
    "print(f\"   - Base model avg entropy: {np.mean(base_ent):.4f}\")\n",
    "print(f\"   - Chat model avg entropy: {np.mean(chat_ent):.4f}\")\n",
    "\n",
    "print(\"\\n3. Assistant Prefix Effects:\")\n",
    "for model_name, results in prefix_results.items():\n",
    "    best = max(results, key=lambda x: x[\"target_log_prob\"])\n",
    "    worst = min(results, key=lambda x: x[\"target_log_prob\"])\n",
    "    print(f\"   {model_name}: best='{best['prefix']}', worst='{worst['prefix']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Save sensitivity data\n",
    "sens_df.to_csv('../results/exp3_sensitivity.csv', index=False)\n",
    "\n",
    "# Save prefix results\n",
    "prefix_save = {}\n",
    "for model_name, results in prefix_results.items():\n",
    "    prefix_save[model_name] = [\n",
    "        {\"prefix\": r[\"prefix\"], \"log_prob\": r[\"target_log_prob\"], \"entropy\": r[\"entropy\"]}\n",
    "        for r in results\n",
    "    ]\n",
    "\n",
    "with open('../results/exp3_prefix_results.json', 'w') as f:\n",
    "    json.dump(prefix_save, f, indent=2)\n",
    "\n",
    "print(\"Results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
