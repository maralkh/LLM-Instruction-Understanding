{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: Few-Shot Learning Analysis\n",
    "\n",
    "**Goal:** Understand how few-shot examples affect model behavior.\n",
    "\n",
    "**Key Questions:**\n",
    "- How does the number of examples affect performance?\n",
    "- Does example order matter?\n",
    "- What makes a \"good\" example vs a \"bad\" one?\n",
    "- Is it the content or the format that helps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.prompt_utils import FewShotExample, FewShotPromptBuilder\n",
    "from src.metrics import ExperimentResults, SequenceMetrics\n",
    "from src.visualization import set_style\n",
    "\n",
    "set_style()\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Few-Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment classification examples\n",
    "SENTIMENT_EXAMPLES = [\n",
    "    FewShotExample(\"This movie was fantastic!\", \"positive\"),\n",
    "    FewShotExample(\"I absolutely hated it.\", \"negative\"),\n",
    "    FewShotExample(\"Best purchase I ever made.\", \"positive\"),\n",
    "    FewShotExample(\"Complete waste of money.\", \"negative\"),\n",
    "    FewShotExample(\"The service was excellent!\", \"positive\"),\n",
    "    FewShotExample(\"Terrible experience, never again.\", \"negative\"),\n",
    "    FewShotExample(\"I'm so happy with this!\", \"positive\"),\n",
    "    FewShotExample(\"Very disappointing product.\", \"negative\"),\n",
    "]\n",
    "\n",
    "# Math examples\n",
    "MATH_EXAMPLES = [\n",
    "    FewShotExample(\"2 + 3\", \"5\"),\n",
    "    FewShotExample(\"10 - 4\", \"6\"),\n",
    "    FewShotExample(\"5 * 3\", \"15\"),\n",
    "    FewShotExample(\"20 / 4\", \"5\"),\n",
    "    FewShotExample(\"7 + 8\", \"15\"),\n",
    "    FewShotExample(\"15 - 9\", \"6\"),\n",
    "]\n",
    "\n",
    "# Test queries\n",
    "TEST_QUERIES = {\n",
    "    \"sentiment\": {\n",
    "        \"examples\": SENTIMENT_EXAMPLES,\n",
    "        \"query\": \"This restaurant exceeded all my expectations!\",\n",
    "        \"expected\": \"positive\",\n",
    "        \"format\": \"Input: {input}\\nSentiment: {output}\",\n",
    "        \"query_format\": \"Input: {query}\\nSentiment:\"\n",
    "    },\n",
    "    \"math\": {\n",
    "        \"examples\": MATH_EXAMPLES,\n",
    "        \"query\": \"9 + 6\",\n",
    "        \"expected\": \"15\",\n",
    "        \"format\": \"Problem: {input}\\nAnswer: {output}\",\n",
    "        \"query_format\": \"Problem: {query}\\nAnswer:\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N-Shot Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_n_shot_scaling(model, task_data, n_values):\n",
    "    \"\"\"Test how performance scales with number of examples.\"\"\"\n",
    "    builder = FewShotPromptBuilder(task_data[\"examples\"])\n",
    "    \n",
    "    results = []\n",
    "    for n in n_values:\n",
    "        prompt = builder.build(\n",
    "            query=task_data[\"query\"],\n",
    "            n_examples=n,\n",
    "            example_format=task_data[\"format\"],\n",
    "            query_format=task_data[\"query_format\"]\n",
    "        )\n",
    "        \n",
    "        dist = model.get_next_token_distribution(prompt)\n",
    "        seq_probs = model.get_sequence_log_probs(prompt, \" \" + task_data[\"expected\"])\n",
    "        \n",
    "        results.append({\n",
    "            \"n_shot\": n,\n",
    "            \"prompt\": prompt,\n",
    "            \"target_log_prob\": seq_probs[\"total_log_prob\"],\n",
    "            \"entropy\": dist[\"entropy\"],\n",
    "            \"top_5\": dist[\"top_tokens\"][:5],\n",
    "            \"prompt_length\": len(prompt)\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test n-shot scaling\n",
    "n_values = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "scaling_results = {}\n",
    "for task_name, task_data in TEST_QUERIES.items():\n",
    "    max_n = min(max(n_values), len(task_data[\"examples\"]))\n",
    "    valid_n = [n for n in n_values if n <= max_n]\n",
    "    \n",
    "    print(f\"\\nTesting {task_name}...\")\n",
    "    results = test_n_shot_scaling(model, task_data, valid_n)\n",
    "    scaling_results[task_name] = results\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"  {r['n_shot']}-shot: log_prob={r['target_log_prob']:.3f}, entropy={r['entropy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize n-shot scaling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for task_name, results in scaling_results.items():\n",
    "    n_shots = [r[\"n_shot\"] for r in results]\n",
    "    log_probs = [r[\"target_log_prob\"] for r in results]\n",
    "    \n",
    "    axes[0].plot(n_shots, log_probs, 'o-', label=task_name, linewidth=2, markersize=8)\n",
    "\n",
    "axes[0].set_xlabel('Number of Examples')\n",
    "axes[0].set_ylabel('Target Log Probability')\n",
    "axes[0].set_title('N-Shot Scaling: Target Probability')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "for task_name, results in scaling_results.items():\n",
    "    n_shots = [r[\"n_shot\"] for r in results]\n",
    "    entropies = [r[\"entropy\"] for r in results]\n",
    "    \n",
    "    axes[1].plot(n_shots, entropies, 'o-', label=task_name, linewidth=2, markersize=8)\n",
    "\n",
    "axes[1].set_xlabel('Number of Examples')\n",
    "axes[1].set_ylabel('Output Entropy')\n",
    "axes[1].set_title('N-Shot Scaling: Output Entropy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp4_nshot_scaling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example Order Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_example_order(model, task_data, n_examples=4, n_permutations=10):\n",
    "    \"\"\"Test how example order affects performance.\"\"\"\n",
    "    examples = task_data[\"examples\"][:n_examples]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Test multiple random orderings\n",
    "    for i in range(n_permutations):\n",
    "        shuffled = examples.copy()\n",
    "        random.shuffle(shuffled)\n",
    "        \n",
    "        builder = FewShotPromptBuilder(shuffled)\n",
    "        prompt = builder.build(\n",
    "            query=task_data[\"query\"],\n",
    "            n_examples=n_examples,\n",
    "            example_format=task_data[\"format\"],\n",
    "            query_format=task_data[\"query_format\"]\n",
    "        )\n",
    "        \n",
    "        seq_probs = model.get_sequence_log_probs(prompt, \" \" + task_data[\"expected\"])\n",
    "        dist = model.get_next_token_distribution(prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"order\": [ex.input[:20] for ex in shuffled],\n",
    "            \"target_log_prob\": seq_probs[\"total_log_prob\"],\n",
    "            \"entropy\": dist[\"entropy\"]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test order effects\n",
    "order_results = {}\n",
    "\n",
    "for task_name, task_data in TEST_QUERIES.items():\n",
    "    print(f\"\\nTesting order effects for {task_name}...\")\n",
    "    results = test_example_order(model, task_data, n_examples=4, n_permutations=20)\n",
    "    order_results[task_name] = results\n",
    "    \n",
    "    log_probs = [r[\"target_log_prob\"] for r in results]\n",
    "    print(f\"  Log-prob range: [{min(log_probs):.3f}, {max(log_probs):.3f}]\")\n",
    "    print(f\"  Variance: {np.var(log_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize order effects\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "data_for_plot = []\n",
    "labels = []\n",
    "for task_name, results in order_results.items():\n",
    "    log_probs = [r[\"target_log_prob\"] for r in results]\n",
    "    data_for_plot.append(log_probs)\n",
    "    labels.append(task_name)\n",
    "\n",
    "bp = ax.boxplot(data_for_plot, labels=labels, patch_artist=True)\n",
    "colors = ['steelblue', 'coral']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('Target Log Probability')\n",
    "ax.set_title('Effect of Example Order (20 random permutations each)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp4_order_effects.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_individual_examples(model, task_data):\n",
    "    \"\"\"Test the impact of each individual example.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, example in enumerate(task_data[\"examples\"]):\n",
    "        # Single example prompt\n",
    "        builder = FewShotPromptBuilder([example])\n",
    "        prompt = builder.build(\n",
    "            query=task_data[\"query\"],\n",
    "            n_examples=1,\n",
    "            example_format=task_data[\"format\"],\n",
    "            query_format=task_data[\"query_format\"]\n",
    "        )\n",
    "        \n",
    "        seq_probs = model.get_sequence_log_probs(prompt, \" \" + task_data[\"expected\"])\n",
    "        dist = model.get_next_token_distribution(prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"example_idx\": i,\n",
    "            \"example_input\": example.input,\n",
    "            \"example_output\": example.output,\n",
    "            \"target_log_prob\": seq_probs[\"total_log_prob\"],\n",
    "            \"entropy\": dist[\"entropy\"]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual example quality\n",
    "example_quality = {}\n",
    "\n",
    "for task_name, task_data in TEST_QUERIES.items():\n",
    "    print(f\"\\nAnalyzing examples for {task_name}...\")\n",
    "    results = test_individual_examples(model, task_data)\n",
    "    example_quality[task_name] = results\n",
    "    \n",
    "    sorted_results = sorted(results, key=lambda x: x[\"target_log_prob\"], reverse=True)\n",
    "    print(\"\\nExample ranking (best to worst):\")\n",
    "    for r in sorted_results:\n",
    "        print(f\"  '{r['example_input'][:30]}...' -> {r['example_output']}: log_prob={r['target_log_prob']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format vs Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_format_vs_content(model, task_data):\n",
    "    \"\"\"Disentangle format from content by using different combinations.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # 1. Full examples (content + format)\n",
    "    builder = FewShotPromptBuilder(task_data[\"examples\"][:3])\n",
    "    full_prompt = builder.build(\n",
    "        query=task_data[\"query\"],\n",
    "        n_examples=3,\n",
    "        example_format=task_data[\"format\"],\n",
    "        query_format=task_data[\"query_format\"]\n",
    "    )\n",
    "    \n",
    "    # 2. Format only (use placeholder content)\n",
    "    placeholder_examples = [\n",
    "        FewShotExample(\"[example input]\", \"[output]\"),\n",
    "        FewShotExample(\"[example input]\", \"[output]\"),\n",
    "        FewShotExample(\"[example input]\", \"[output]\"),\n",
    "    ]\n",
    "    builder_format = FewShotPromptBuilder(placeholder_examples)\n",
    "    format_prompt = builder_format.build(\n",
    "        query=task_data[\"query\"],\n",
    "        n_examples=3,\n",
    "        example_format=task_data[\"format\"],\n",
    "        query_format=task_data[\"query_format\"]\n",
    "    )\n",
    "    \n",
    "    # 3. Wrong labels (content with incorrect outputs)\n",
    "    wrong_examples = []\n",
    "    for ex in task_data[\"examples\"][:3]:\n",
    "        wrong_output = \"negative\" if ex.output == \"positive\" else \"positive\"\n",
    "        if task_data == TEST_QUERIES[\"math\"]:\n",
    "            wrong_output = str(int(ex.output) + 1) if ex.output.isdigit() else \"wrong\"\n",
    "        wrong_examples.append(FewShotExample(ex.input, wrong_output))\n",
    "    \n",
    "    builder_wrong = FewShotPromptBuilder(wrong_examples)\n",
    "    wrong_prompt = builder_wrong.build(\n",
    "        query=task_data[\"query\"],\n",
    "        n_examples=3,\n",
    "        example_format=task_data[\"format\"],\n",
    "        query_format=task_data[\"query_format\"]\n",
    "    )\n",
    "    \n",
    "    # Evaluate all\n",
    "    for name, prompt in [(\"full\", full_prompt), (\"format_only\", format_prompt), (\"wrong_labels\", wrong_prompt)]:\n",
    "        seq_probs = model.get_sequence_log_probs(prompt, \" \" + task_data[\"expected\"])\n",
    "        dist = model.get_next_token_distribution(prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"condition\": name,\n",
    "            \"target_log_prob\": seq_probs[\"total_log_prob\"],\n",
    "            \"entropy\": dist[\"entropy\"]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test format vs content\n",
    "format_content_results = {}\n",
    "\n",
    "for task_name, task_data in TEST_QUERIES.items():\n",
    "    print(f\"\\n{task_name}:\")\n",
    "    results = test_format_vs_content(model, task_data)\n",
    "    format_content_results[task_name] = results\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"  {r['condition']:15s}: log_prob={r['target_log_prob']:.3f}, entropy={r['entropy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize format vs content\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(TEST_QUERIES))\n",
    "width = 0.25\n",
    "\n",
    "conditions = [\"full\", \"format_only\", \"wrong_labels\"]\n",
    "colors = ['green', 'steelblue', 'red']\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    values = []\n",
    "    for task_name in TEST_QUERIES.keys():\n",
    "        for r in format_content_results[task_name]:\n",
    "            if r[\"condition\"] == condition:\n",
    "                values.append(r[\"target_log_prob\"])\n",
    "    \n",
    "    ax.bar(x + i*width, values, width, label=condition, color=colors[i], alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Task')\n",
    "ax.set_ylabel('Target Log Probability')\n",
    "ax.set_title('Format vs Content: What Makes Few-Shot Work?')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(TEST_QUERIES.keys())\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp4_format_vs_content.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 4 SUMMARY: Few-Shot Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. N-Shot Scaling:\")\n",
    "for task_name, results in scaling_results.items():\n",
    "    best_n = max(results, key=lambda x: x[\"target_log_prob\"])\n",
    "    print(f\"   {task_name}: Best at {best_n['n_shot']}-shot (log_prob={best_n['target_log_prob']:.3f})\")\n",
    "\n",
    "print(\"\\n2. Order Sensitivity:\")\n",
    "for task_name, results in order_results.items():\n",
    "    log_probs = [r[\"target_log_prob\"] for r in results]\n",
    "    print(f\"   {task_name}: variance={np.var(log_probs):.4f}, range={max(log_probs)-min(log_probs):.3f}\")\n",
    "\n",
    "print(\"\\n3. Format vs Content:\")\n",
    "for task_name in TEST_QUERIES.keys():\n",
    "    results = format_content_results[task_name]\n",
    "    full = next(r for r in results if r[\"condition\"] == \"full\")\n",
    "    format_only = next(r for r in results if r[\"condition\"] == \"format_only\")\n",
    "    wrong = next(r for r in results if r[\"condition\"] == \"wrong_labels\")\n",
    "    \n",
    "    format_contrib = format_only[\"target_log_prob\"]\n",
    "    content_contrib = full[\"target_log_prob\"] - format_only[\"target_log_prob\"]\n",
    "    \n",
    "    print(f\"   {task_name}: format contribution ≈ {format_contrib:.3f}, content adds ≈ {content_contrib:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "save_data = {\n",
    "    \"scaling\": {k: [{\"n_shot\": r[\"n_shot\"], \"log_prob\": r[\"target_log_prob\"]} for r in v] \n",
    "                for k, v in scaling_results.items()},\n",
    "    \"order_variance\": {k: np.var([r[\"target_log_prob\"] for r in v]) \n",
    "                       for k, v in order_results.items()},\n",
    "    \"format_vs_content\": format_content_results\n",
    "}\n",
    "\n",
    "with open('../results/exp4_fewshot_results.json', 'w') as f:\n",
    "    json.dump(save_data, f, indent=2, default=float)\n",
    "\n",
    "print(\"Results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
