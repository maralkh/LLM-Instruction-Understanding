{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7: Learning Dynamics - Training Distribution Hypothesis\n",
    "\n",
    "**Goal:** Understand if prompt effectiveness relates to training distribution patterns.\n",
    "\n",
    "**Hypothesis:** Effective prompts \"activate\" patterns from training data (StackOverflow, textbooks, Wikipedia, etc.)\n",
    "\n",
    "**Key Questions:**\n",
    "- Do prompts mimicking web text formats perform better?\n",
    "- Do \"unnatural\" prompt formats fail predictably?\n",
    "- Can we identify which training sources a prompt is activating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.metrics import ExperimentResults, SequenceMetrics\n",
    "from src.visualization import set_style\n",
    "\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Format Templates from Known Sources\n",
    "\n",
    "We create prompts that mimic formats likely seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates mimicking known web/training data formats\n",
    "FORMAT_TEMPLATES = {\n",
    "    # StackOverflow-style\n",
    "    \"stackoverflow\": {\n",
    "        \"template\": \"\"\"**Question:**\n",
    "{question}\n",
    "\n",
    "**Answer:**\"\"\",\n",
    "        \"description\": \"StackOverflow Q&A format\"\n",
    "    },\n",
    "    \n",
    "    # Wikipedia-style\n",
    "    \"wikipedia\": {\n",
    "        \"template\": \"\"\"{question}\n",
    "\n",
    "According to available sources,\"\"\",\n",
    "        \"description\": \"Wikipedia expository style\"\n",
    "    },\n",
    "    \n",
    "    # Textbook-style\n",
    "    \"textbook\": {\n",
    "        \"template\": \"\"\"**Example Problem:**\n",
    "{question}\n",
    "\n",
    "**Solution:**\n",
    "To solve this problem,\"\"\",\n",
    "        \"description\": \"Textbook problem/solution format\"\n",
    "    },\n",
    "    \n",
    "    # Reddit-style\n",
    "    \"reddit\": {\n",
    "        \"template\": \"\"\"{question}\n",
    "\n",
    "Edit: Figured it out!\"\"\",\n",
    "        \"description\": \"Reddit discussion style\"\n",
    "    },\n",
    "    \n",
    "    # Documentation-style\n",
    "    \"documentation\": {\n",
    "        \"template\": \"\"\"## Overview\n",
    "\n",
    "{question}\n",
    "\n",
    "## Answer\n",
    "\n",
    "The answer is\"\"\",\n",
    "        \"description\": \"Technical documentation format\"\n",
    "    },\n",
    "    \n",
    "    # Chat/conversation style\n",
    "    \"chat\": {\n",
    "        \"template\": \"\"\"User: {question}\n",
    "Assistant:\"\"\",\n",
    "        \"description\": \"Chat conversation format\"\n",
    "    },\n",
    "    \n",
    "    # Academic paper style\n",
    "    \"academic\": {\n",
    "        \"template\": \"\"\"Abstract: This paper addresses the question: {question}\n",
    "\n",
    "Results: Our analysis shows that\"\"\",\n",
    "        \"description\": \"Academic paper style\"\n",
    "    },\n",
    "    \n",
    "    # JSON-style\n",
    "    \"json\": {\n",
    "        \"template\": \"\"\"{{\n",
    "  \"question\": \"{question}\",\n",
    "  \"answer\": \"\"\"\",\n",
    "        \"description\": \"JSON data format\"\n",
    "    },\n",
    "    \n",
    "    # Plain/minimal\n",
    "    \"plain\": {\n",
    "        \"template\": \"{question}\",\n",
    "        \"description\": \"No formatting, just the question\"\n",
    "    },\n",
    "    \n",
    "    # Unnatural/adversarial formats\n",
    "    \"reversed\": {\n",
    "        \"template\": \"{question_reversed}\",\n",
    "        \"description\": \"Reversed text (unnatural)\"\n",
    "    },\n",
    "    \n",
    "    \"leetspeak\": {\n",
    "        \"template\": \"{question_leet}\",\n",
    "        \"description\": \"Leetspeak substitution (semi-natural)\"\n",
    "    },\n",
    "    \n",
    "    \"uppercase\": {\n",
    "        \"template\": \"{question_upper}\",\n",
    "        \"description\": \"All uppercase (semi-natural)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test questions\n",
    "TEST_QUESTIONS = [\n",
    "    {\"q\": \"What is the sum of 45 and 67?\", \"a\": \"112\"},\n",
    "    {\"q\": \"What is the capital of Germany?\", \"a\": \"Berlin\"},\n",
    "    {\"q\": \"What color do you get by mixing red and blue?\", \"a\": \"purple\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Format Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_format_variants(question):\n",
    "    \"\"\"Create all format variants for a question.\"\"\"\n",
    "    variants = {}\n",
    "    \n",
    "    # Standard variants\n",
    "    for name, fmt in FORMAT_TEMPLATES.items():\n",
    "        if name == \"reversed\":\n",
    "            variants[name] = question[::-1]\n",
    "        elif name == \"leetspeak\":\n",
    "            leet_map = {'a': '4', 'e': '3', 'i': '1', 'o': '0', 's': '5', 't': '7'}\n",
    "            leet_q = ''.join(leet_map.get(c.lower(), c) for c in question)\n",
    "            variants[name] = leet_q\n",
    "        elif name == \"uppercase\":\n",
    "            variants[name] = question.upper()\n",
    "        else:\n",
    "            variants[name] = fmt[\"template\"].format(question=question)\n",
    "    \n",
    "    return variants\n",
    "\n",
    "\n",
    "def test_formats(model, question, expected):\n",
    "    \"\"\"Test all format variants for a question.\"\"\"\n",
    "    variants = create_format_variants(question)\n",
    "    results = []\n",
    "    \n",
    "    for name, prompt in variants.items():\n",
    "        dist = model.get_next_token_distribution(prompt)\n",
    "        seq_probs = model.get_sequence_log_probs(prompt, \" \" + expected)\n",
    "        \n",
    "        results.append({\n",
    "            \"format\": name,\n",
    "            \"description\": FORMAT_TEMPLATES.get(name, {}).get(\"description\", name),\n",
    "            \"prompt\": prompt,\n",
    "            \"target_log_prob\": seq_probs[\"total_log_prob\"],\n",
    "            \"entropy\": dist[\"entropy\"],\n",
    "            \"top_5\": dist[\"top_tokens\"][:5],\n",
    "            \"prompt_length\": len(prompt)\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run format testing\n",
    "all_format_results = []\n",
    "\n",
    "for test in tqdm(TEST_QUESTIONS, desc=\"Testing questions\"):\n",
    "    results = test_formats(model, test[\"q\"], test[\"a\"])\n",
    "    for r in results:\n",
    "        r[\"question\"] = test[\"q\"]\n",
    "        r[\"expected\"] = test[\"a\"]\n",
    "    all_format_results.extend(results)\n",
    "\n",
    "df = pd.DataFrame(all_format_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze format effectiveness\n",
    "print(\"=== Format Effectiveness Ranking ===\")\n",
    "\n",
    "format_perf = df.groupby('format')['target_log_prob'].agg(['mean', 'std']).sort_values('mean', ascending=False)\n",
    "\n",
    "print(\"\\nRanked by target log-probability:\")\n",
    "for fmt, row in format_perf.iterrows():\n",
    "    desc = FORMAT_TEMPLATES.get(fmt, {}).get(\"description\", fmt)\n",
    "    print(f\"  {fmt:15s} ({desc:30s}): {row['mean']:.4f} ± {row['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "formats = format_perf.index.tolist()\n",
    "means = format_perf['mean'].values\n",
    "stds = format_perf['std'].values\n",
    "\n",
    "# Color by category\n",
    "natural_formats = ['stackoverflow', 'wikipedia', 'textbook', 'documentation', 'chat', 'academic', 'json']\n",
    "colors = ['green' if f in natural_formats else 'orange' if f == 'plain' else 'red' for f in formats]\n",
    "\n",
    "ax.barh(range(len(formats)), means, xerr=stds, color=colors, alpha=0.7, capsize=3)\n",
    "ax.set_yticks(range(len(formats)))\n",
    "ax.set_yticklabels(formats)\n",
    "ax.set_xlabel('Target Log Probability')\n",
    "ax.set_title('Format Effectiveness: Training Distribution Hypothesis\\n(Green=Natural web formats, Red=Unnatural)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp7_format_effectiveness.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naturalness Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prompt_perplexity(model, prompt):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of the prompt itself.\n",
    "    Lower perplexity = more \"natural\" to the model.\n",
    "    \"\"\"\n",
    "    # Get log probs for the prompt tokens\n",
    "    tokens = model.tokenizer.encode(prompt)\n",
    "    if len(tokens) < 2:\n",
    "        return float('inf')\n",
    "    \n",
    "    # Compute perplexity by predicting each token\n",
    "    import torch\n",
    "    \n",
    "    inputs = model.tokenizer(prompt, return_tensors=\"pt\").to(model.config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Shift for next-token prediction\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = inputs.input_ids[..., 1:].contiguous()\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    \n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity for each format\n",
    "format_perplexities = {}\n",
    "\n",
    "test_q = TEST_QUESTIONS[0][\"q\"]\n",
    "variants = create_format_variants(test_q)\n",
    "\n",
    "print(\"=== Format Perplexity (Lower = More Natural) ===\")\n",
    "for name, prompt in tqdm(variants.items(), desc=\"Computing perplexities\"):\n",
    "    ppl = compute_prompt_perplexity(model, prompt)\n",
    "    format_perplexities[name] = ppl\n",
    "\n",
    "# Sort by perplexity\n",
    "sorted_ppl = sorted(format_perplexities.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nRanked by naturalness (lower perplexity = more natural):\")\n",
    "for name, ppl in sorted_ppl:\n",
    "    print(f\"  {name:15s}: perplexity = {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate perplexity with effectiveness\n",
    "effectiveness = dict(zip(format_perf.index, format_perf['mean']))\n",
    "\n",
    "# Match formats\n",
    "matched_data = []\n",
    "for fmt in format_perplexities:\n",
    "    if fmt in effectiveness:\n",
    "        matched_data.append({\n",
    "            \"format\": fmt,\n",
    "            \"perplexity\": format_perplexities[fmt],\n",
    "            \"effectiveness\": effectiveness[fmt]\n",
    "        })\n",
    "\n",
    "matched_df = pd.DataFrame(matched_data)\n",
    "\n",
    "# Calculate correlation\n",
    "corr = matched_df[\"perplexity\"].corr(matched_df[\"effectiveness\"])\n",
    "print(f\"\\nCorrelation between perplexity and effectiveness: {corr:.4f}\")\n",
    "print(f\"Interpretation: {'Lower perplexity (more natural) → better performance' if corr < 0 else 'Perplexity not strongly predictive'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.scatter(matched_df[\"perplexity\"], matched_df[\"effectiveness\"], s=100, alpha=0.7)\n",
    "\n",
    "for _, row in matched_df.iterrows():\n",
    "    ax.annotate(row[\"format\"], (row[\"perplexity\"], row[\"effectiveness\"]),\n",
    "                textcoords=\"offset points\", xytext=(5,5), fontsize=8)\n",
    "\n",
    "ax.set_xlabel(\"Prompt Perplexity (Lower = More Natural)\")\n",
    "ax.set_ylabel(\"Target Log Probability (Higher = Better)\")\n",
    "ax.set_title(f\"Naturalness vs Effectiveness (r={corr:.3f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp7_naturalness_vs_effectiveness.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Source-Specific Triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test source-specific trigger phrases\n",
    "SOURCE_TRIGGERS = {\n",
    "    \"stackoverflow\": [\n",
    "        \"**Question:**\", \"**Answer:**\", \"Edit:\", \"Update:\", \"SOLVED:\"\n",
    "    ],\n",
    "    \"wikipedia\": [\n",
    "        \"According to\", \"is defined as\", \"refers to\", \"[citation needed]\"\n",
    "    ],\n",
    "    \"academic\": [\n",
    "        \"Abstract:\", \"Introduction:\", \"Methods:\", \"Results:\", \"et al.\"\n",
    "    ],\n",
    "    \"documentation\": [\n",
    "        \"## Overview\", \"### Parameters\", \"Returns:\", \"Example:\", \"Usage:\"\n",
    "    ],\n",
    "    \"code\": [\n",
    "        \"```python\", \"def \", \"return \", \"# \", \"import \"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def test_trigger_phrases(model, question, expected, triggers):\n",
    "    \"\"\"Test effectiveness of adding source-specific triggers.\"\"\"\n",
    "    baseline = model.get_sequence_log_probs(question, \" \" + expected)[\"total_log_prob\"]\n",
    "    \n",
    "    results = [{\"source\": \"baseline\", \"trigger\": \"none\", \"improvement\": 0, \"log_prob\": baseline}]\n",
    "    \n",
    "    for source, trigger_list in triggers.items():\n",
    "        for trigger in trigger_list:\n",
    "            prompt = f\"{trigger}\\n{question}\"\n",
    "            log_prob = model.get_sequence_log_probs(prompt, \" \" + expected)[\"total_log_prob\"]\n",
    "            \n",
    "            results.append({\n",
    "                \"source\": source,\n",
    "                \"trigger\": trigger,\n",
    "                \"log_prob\": log_prob,\n",
    "                \"improvement\": log_prob - baseline\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test triggers\n",
    "trigger_results = []\n",
    "\n",
    "for test in TEST_QUESTIONS:\n",
    "    results = test_trigger_phrases(model, test[\"q\"], test[\"a\"], SOURCE_TRIGGERS)\n",
    "    for r in results:\n",
    "        r[\"question\"] = test[\"q\"]\n",
    "    trigger_results.extend(results)\n",
    "\n",
    "trigger_df = pd.DataFrame(trigger_results)\n",
    "\n",
    "# Analyze by source\n",
    "print(\"=== Source-Specific Trigger Effectiveness ===\")\n",
    "source_perf = trigger_df[trigger_df['source'] != 'baseline'].groupby('source')['improvement'].mean().sort_values(ascending=False)\n",
    "\n",
    "for source, improvement in source_perf.items():\n",
    "    print(f\"  {source:15s}: avg improvement = {improvement:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 7 SUMMARY: Training Distribution Hypothesis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Format Effectiveness Ranking (top 5):\")\n",
    "for i, (fmt, row) in enumerate(format_perf.head(5).iterrows()):\n",
    "    print(f\"   {i+1}. {fmt}: {row['mean']:.4f}\")\n",
    "\n",
    "print(\"\\n2. Naturalness-Effectiveness Correlation:\")\n",
    "print(f\"   r = {corr:.4f}\")\n",
    "if corr < -0.3:\n",
    "    print(\"   → Strong support: More natural formats work better\")\n",
    "elif corr > 0.3:\n",
    "    print(\"   → Counter-intuitive: Less natural formats work better\")\n",
    "else:\n",
    "    print(\"   → Weak relationship: Naturalness alone doesn't predict effectiveness\")\n",
    "\n",
    "print(\"\\n3. Best Source-Specific Triggers:\")\n",
    "for source, improvement in source_perf.head(3).items():\n",
    "    print(f\"   {source}: {improvement:+.4f}\")\n",
    "\n",
    "print(\"\\n4. Key Insight:\")\n",
    "print(\"   [Fill after running: Do training-like formats help?]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "save_data = {\n",
    "    \"format_effectiveness\": format_perf.to_dict(),\n",
    "    \"format_perplexities\": format_perplexities,\n",
    "    \"naturalness_correlation\": float(corr),\n",
    "    \"source_trigger_effectiveness\": source_perf.to_dict()\n",
    "}\n",
    "\n",
    "with open('../results/exp7_training_distribution_results.json', 'w') as f:\n",
    "    json.dump(save_data, f, indent=2, default=float)\n",
    "\n",
    "print(\"Results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
