{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Sampling Consistency Analysis\n",
    "\n",
    "**Goal:** Analyze how system prompts affect behavioral consistency across repeated nucleus sampling.\n",
    "\n",
    "**Key Questions:**\n",
    "- Do certain system prompts lead to more consistent/deterministic outputs?\n",
    "- How does output variance change with different instruction types?\n",
    "- Is there a correlation between entropy and actual output diversity?\n",
    "- Do some prompts stabilize behavior while others introduce more randomness?\n",
    "\n",
    "**Metrics:**\n",
    "- Output token overlap across samples\n",
    "- Semantic similarity (via embeddings)\n",
    "- Length variance\n",
    "- First token consistency\n",
    "- Response structure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    import shutil\n",
    "    if os.path.exists('/content/LLM-Instruction-Understanding'):\n",
    "        shutil.rmtree('/content/LLM-Instruction-Understanding')\n",
    "    !git clone https://github.com/maralkh/LLM-Instruction-Understanding.git\n",
    "    os.chdir('/content/LLM-Instruction-Understanding')\n",
    "    !pip install -q -r requirements.txt\n",
    "    sys.path.insert(0, '/content/LLM-Instruction-Understanding')\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.test_configs import get_all_test_prompts, get_core_system_prompts, get_system_prompts, build_chat_prompt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "layer_info = model.get_layer_info()\n",
    "print(f\"Model: {layer_info['model_name']}\")\n",
    "print(f\"Layers: {layer_info['n_layers']}, Heads: {layer_info['n_heads']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Nucleus Sampling with Multiple Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(\n",
    "    model, \n",
    "    prompt: str, \n",
    "    n_samples: int = 10,\n",
    "    max_new_tokens: int = 50,\n",
    "    top_p: float = 0.9,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 50\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate multiple samples using nucleus sampling.\n",
    "    Returns list of dicts with generated text, tokens, and metadata.\n",
    "    \"\"\"\n",
    "    inputs = model.tokenizer(prompt, return_tensors=\"pt\").to(model.config.device)\n",
    "    prompt_length = inputs.input_ids.shape[1]\n",
    "    \n",
    "    samples = []\n",
    "    for i in range(n_samples):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=model.tokenizer.pad_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        generated_ids = outputs.sequences[0, prompt_length:]\n",
    "        generated_text = model.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        generated_tokens = [model.tokenizer.decode([t]) for t in generated_ids]\n",
    "        \n",
    "        # Calculate per-token entropy from scores\n",
    "        entropies = []\n",
    "        for score in outputs.scores:\n",
    "            probs = F.softmax(score[0].float(), dim=-1)\n",
    "            ent = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "            entropies.append(ent if np.isfinite(ent) else 0.0)\n",
    "        \n",
    "        samples.append({\n",
    "            'text': generated_text,\n",
    "            'tokens': generated_tokens,\n",
    "            'token_ids': generated_ids.cpu().tolist(),\n",
    "            'length': len(generated_ids),\n",
    "            'first_token': generated_tokens[0] if generated_tokens else '',\n",
    "            'mean_entropy': np.mean(entropies) if entropies else 0.0,\n",
    "            'entropies': entropies\n",
    "        })\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_consistency(samples: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute consistency metrics across multiple samples.\n",
    "    \"\"\"\n",
    "    n = len(samples)\n",
    "    if n == 0:\n",
    "        return {}\n",
    "    \n",
    "    # 1. First token consistency\n",
    "    first_tokens = [s['first_token'] for s in samples]\n",
    "    first_token_counts = Counter(first_tokens)\n",
    "    most_common_first = first_token_counts.most_common(1)[0][1] / n\n",
    "    first_token_entropy = -sum((c/n) * np.log(c/n + 1e-10) for c in first_token_counts.values())\n",
    "    \n",
    "    # 2. Length consistency\n",
    "    lengths = [s['length'] for s in samples]\n",
    "    length_mean = np.mean(lengths)\n",
    "    length_std = np.std(lengths)\n",
    "    length_cv = length_std / (length_mean + 1e-10)  # Coefficient of variation\n",
    "    \n",
    "    # 3. Token overlap (Jaccard similarity between all pairs)\n",
    "    jaccard_scores = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            set1 = set(samples[i]['token_ids'])\n",
    "            set2 = set(samples[j]['token_ids'])\n",
    "            intersection = len(set1 & set2)\n",
    "            union = len(set1 | set2)\n",
    "            jaccard = intersection / union if union > 0 else 0\n",
    "            jaccard_scores.append(jaccard)\n",
    "    mean_jaccard = np.mean(jaccard_scores) if jaccard_scores else 0\n",
    "    \n",
    "    # 4. N-gram overlap (bigrams)\n",
    "    def get_ngrams(tokens, n=2):\n",
    "        return set(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1))\n",
    "    \n",
    "    bigram_overlaps = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            bg1 = get_ngrams(samples[i]['token_ids'], 2)\n",
    "            bg2 = get_ngrams(samples[j]['token_ids'], 2)\n",
    "            if len(bg1 | bg2) > 0:\n",
    "                overlap = len(bg1 & bg2) / len(bg1 | bg2)\n",
    "                bigram_overlaps.append(overlap)\n",
    "    mean_bigram_overlap = np.mean(bigram_overlaps) if bigram_overlaps else 0\n",
    "    \n",
    "    # 5. Unique outputs ratio\n",
    "    unique_texts = len(set(s['text'] for s in samples))\n",
    "    unique_ratio = unique_texts / n\n",
    "    \n",
    "    # 6. Mean generation entropy\n",
    "    mean_entropy = np.mean([s['mean_entropy'] for s in samples])\n",
    "    \n",
    "    # 7. Text similarity (character-level)\n",
    "    def levenshtein_ratio(s1, s2):\n",
    "        \"\"\"Simplified similarity based on common prefix/suffix.\"\"\"\n",
    "        if not s1 or not s2:\n",
    "            return 0.0\n",
    "        # Common prefix length\n",
    "        prefix_len = 0\n",
    "        for c1, c2 in zip(s1, s2):\n",
    "            if c1 == c2:\n",
    "                prefix_len += 1\n",
    "            else:\n",
    "                break\n",
    "        return prefix_len / max(len(s1), len(s2))\n",
    "    \n",
    "    prefix_similarities = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            sim = levenshtein_ratio(samples[i]['text'], samples[j]['text'])\n",
    "            prefix_similarities.append(sim)\n",
    "    mean_prefix_sim = np.mean(prefix_similarities) if prefix_similarities else 0\n",
    "    \n",
    "    return {\n",
    "        'first_token_consistency': most_common_first,\n",
    "        'first_token_entropy': first_token_entropy,\n",
    "        'length_mean': length_mean,\n",
    "        'length_std': length_std,\n",
    "        'length_cv': length_cv,\n",
    "        'token_jaccard': mean_jaccard,\n",
    "        'bigram_overlap': mean_bigram_overlap,\n",
    "        'unique_ratio': unique_ratio,\n",
    "        'mean_entropy': mean_entropy,\n",
    "        'prefix_similarity': mean_prefix_sim,\n",
    "        'n_samples': n\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single prompt\n",
    "test_prompt = \"What is the capital of France?\"\n",
    "full_prompt = build_chat_prompt(\"\", test_prompt, model.tokenizer)\n",
    "\n",
    "print(\"Generating 5 samples...\")\n",
    "samples = generate_samples(model, full_prompt, n_samples=5, max_new_tokens=30)\n",
    "\n",
    "print(\"\\n=== Sample Outputs ===\")\n",
    "for i, s in enumerate(samples):\n",
    "    print(f\"\\n[{i+1}] {s['text'][:100]}...\" if len(s['text']) > 100 else f\"\\n[{i+1}] {s['text']}\")\n",
    "\n",
    "consistency = compute_sample_consistency(samples)\n",
    "print(\"\\n=== Consistency Metrics ===\")\n",
    "for k, v in consistency.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Consistency Across System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "N_SAMPLES = 10  # Samples per prompt\n",
    "MAX_NEW_TOKENS = 40\n",
    "TOP_P = 0.9\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "test_prompts = get_all_test_prompts()[:5]  # Use 5 test prompts\n",
    "system_prompts = get_core_system_prompts()\n",
    "\n",
    "print(f\"Testing {len(test_prompts)} prompts × {len(system_prompts)} system prompts × {N_SAMPLES} samples\")\n",
    "print(f\"Total generations: {len(test_prompts) * len(system_prompts) * N_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_consistency_results = []\n",
    "all_samples_data = []  # Store raw samples for deeper analysis\n",
    "\n",
    "for test in tqdm(test_prompts, desc=\"Test prompts\"):\n",
    "    for sys_name, sys_info in system_prompts.items():\n",
    "        try:\n",
    "            full_prompt = build_chat_prompt(sys_info['text'], test['prompt'], model.tokenizer)\n",
    "            \n",
    "            samples = generate_samples(\n",
    "                model, full_prompt, \n",
    "                n_samples=N_SAMPLES,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                top_p=TOP_P,\n",
    "                temperature=TEMPERATURE\n",
    "            )\n",
    "            \n",
    "            consistency = compute_sample_consistency(samples)\n",
    "            consistency['test_id'] = test['id']\n",
    "            consistency['category'] = test['category']\n",
    "            consistency['system_prompt'] = sys_name\n",
    "            \n",
    "            all_consistency_results.append(consistency)\n",
    "            \n",
    "            # Store sample data\n",
    "            for i, s in enumerate(samples):\n",
    "                all_samples_data.append({\n",
    "                    'test_id': test['id'],\n",
    "                    'system_prompt': sys_name,\n",
    "                    'sample_idx': i,\n",
    "                    'text': s['text'],\n",
    "                    'length': s['length'],\n",
    "                    'first_token': s['first_token'],\n",
    "                    'mean_entropy': s['mean_entropy']\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error for {test['id']}/{sys_name}: {e}\")\n",
    "\n",
    "consistency_df = pd.DataFrame(all_consistency_results)\n",
    "samples_df = pd.DataFrame(all_samples_data)\n",
    "print(f\"\\nCollected {len(consistency_df)} consistency measurements\")\n",
    "print(f\"Stored {len(samples_df)} individual samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by system prompt\n",
    "sys_consistency = consistency_df.groupby('system_prompt').agg({\n",
    "    'first_token_consistency': 'mean',\n",
    "    'token_jaccard': 'mean',\n",
    "    'bigram_overlap': 'mean',\n",
    "    'unique_ratio': 'mean',\n",
    "    'length_cv': 'mean',\n",
    "    'mean_entropy': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "# Sort by overall consistency (higher jaccard = more consistent)\n",
    "sys_consistency = sys_consistency.sort_values('token_jaccard', ascending=False)\n",
    "\n",
    "print(\"=== Consistency by System Prompt ===\")\n",
    "print(\"(Higher values = more consistent, except unique_ratio and length_cv)\")\n",
    "print(sys_consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize consistency metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "metrics = [\n",
    "    ('first_token_consistency', 'First Token Consistency', True),\n",
    "    ('token_jaccard', 'Token Jaccard Similarity', True),\n",
    "    ('bigram_overlap', 'Bigram Overlap', True),\n",
    "    ('unique_ratio', 'Unique Output Ratio', False),  # Lower = more consistent\n",
    "    ('length_cv', 'Length Coef. of Variation', False),  # Lower = more consistent\n",
    "    ('mean_entropy', 'Mean Generation Entropy', False)  # Lower = more confident\n",
    "]\n",
    "\n",
    "for ax, (metric, title, higher_better) in zip(axes.flatten(), metrics):\n",
    "    data = sys_consistency[metric].sort_values(ascending=not higher_better)\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(data)))\n",
    "    if not higher_better:\n",
    "        colors = colors[::-1]\n",
    "    \n",
    "    ax.barh(range(len(data)), data.values, color=colors)\n",
    "    ax.set_yticks(range(len(data)))\n",
    "    ax.set_yticklabels(data.index)\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/sampling_consistency_metrics.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Consistency vs Entropy Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is higher entropy associated with lower consistency?\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Entropy vs Token Jaccard\n",
    "ax = axes[0]\n",
    "ax.scatter(consistency_df['mean_entropy'], consistency_df['token_jaccard'], alpha=0.6)\n",
    "corr1, p1 = stats.pearsonr(consistency_df['mean_entropy'], consistency_df['token_jaccard'])\n",
    "ax.set_xlabel('Mean Generation Entropy')\n",
    "ax.set_ylabel('Token Jaccard Similarity')\n",
    "ax.set_title(f'Entropy vs Token Overlap\\nr={corr1:.3f}, p={p1:.3f}')\n",
    "\n",
    "# Entropy vs Unique Ratio\n",
    "ax = axes[1]\n",
    "ax.scatter(consistency_df['mean_entropy'], consistency_df['unique_ratio'], alpha=0.6, color='orange')\n",
    "corr2, p2 = stats.pearsonr(consistency_df['mean_entropy'], consistency_df['unique_ratio'])\n",
    "ax.set_xlabel('Mean Generation Entropy')\n",
    "ax.set_ylabel('Unique Output Ratio')\n",
    "ax.set_title(f'Entropy vs Output Diversity\\nr={corr2:.3f}, p={p2:.3f}')\n",
    "\n",
    "# Entropy vs First Token Consistency\n",
    "ax = axes[2]\n",
    "ax.scatter(consistency_df['mean_entropy'], consistency_df['first_token_consistency'], alpha=0.6, color='green')\n",
    "corr3, p3 = stats.pearsonr(consistency_df['mean_entropy'], consistency_df['first_token_consistency'])\n",
    "ax.set_xlabel('Mean Generation Entropy')\n",
    "ax.set_ylabel('First Token Consistency')\n",
    "ax.set_title(f'Entropy vs First Token\\nr={corr3:.3f}, p={p3:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/entropy_vs_consistency.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Correlation Summary ===\")\n",
    "print(f\"Entropy vs Token Jaccard: r={corr1:.3f} (p={p1:.3f})\")\n",
    "print(f\"Entropy vs Unique Ratio: r={corr2:.3f} (p={p2:.3f})\")\n",
    "print(f\"Entropy vs First Token: r={corr3:.3f} (p={p3:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consistency by Task Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does consistency vary by task type?\n",
    "cat_consistency = consistency_df.groupby('category').agg({\n",
    "    'first_token_consistency': 'mean',\n",
    "    'token_jaccard': 'mean',\n",
    "    'unique_ratio': 'mean',\n",
    "    'mean_entropy': 'mean'\n",
    "}).round(4).sort_values('token_jaccard', ascending=False)\n",
    "\n",
    "print(\"=== Consistency by Task Category ===\")\n",
    "print(cat_consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: System Prompt × Category for Token Jaccard\n",
    "pivot = consistency_df.pivot_table(\n",
    "    values='token_jaccard', \n",
    "    index='system_prompt', \n",
    "    columns='category', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax, vmin=0, vmax=1)\n",
    "ax.set_title('Output Consistency (Token Jaccard) by System Prompt × Category')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/consistency_heatmap.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temperature Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does temperature affect consistency for different system prompts?\n",
    "temperatures = [0.5, 0.7, 1.0, 1.2]\n",
    "test_prompt = test_prompts[0]\n",
    "selected_systems = ['none', 'concise', 'cot', 'expert']\n",
    "\n",
    "temp_results = []\n",
    "\n",
    "for temp in tqdm(temperatures, desc=\"Temperatures\"):\n",
    "    for sys_name in selected_systems:\n",
    "        if sys_name not in system_prompts:\n",
    "            continue\n",
    "        sys_info = system_prompts[sys_name]\n",
    "        full_prompt = build_chat_prompt(sys_info['text'], test_prompt['prompt'], model.tokenizer)\n",
    "        \n",
    "        samples = generate_samples(\n",
    "            model, full_prompt,\n",
    "            n_samples=8,\n",
    "            max_new_tokens=30,\n",
    "            temperature=temp\n",
    "        )\n",
    "        \n",
    "        consistency = compute_sample_consistency(samples)\n",
    "        consistency['temperature'] = temp\n",
    "        consistency['system_prompt'] = sys_name\n",
    "        temp_results.append(consistency)\n",
    "\n",
    "temp_df = pd.DataFrame(temp_results)\n",
    "print(f\"Collected {len(temp_df)} temperature measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, metric in zip(axes, ['token_jaccard', 'unique_ratio', 'first_token_consistency']):\n",
    "    for sys_name in selected_systems:\n",
    "        data = temp_df[temp_df['system_prompt'] == sys_name]\n",
    "        ax.plot(data['temperature'], data[metric], 'o-', label=sys_name, linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Temperature')\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} vs Temperature')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/temperature_sensitivity.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. First Token Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze first token distribution by system prompt\n",
    "first_token_analysis = samples_df.groupby(['system_prompt', 'first_token']).size().reset_index(name='count')\n",
    "\n",
    "# Get top 5 first tokens per system prompt\n",
    "top_first_tokens = first_token_analysis.groupby('system_prompt').apply(\n",
    "    lambda x: x.nlargest(5, 'count')\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"=== Top First Tokens by System Prompt ===\")\n",
    "for sys_name in system_prompts.keys():\n",
    "    sys_data = top_first_tokens[top_first_tokens['system_prompt'] == sys_name]\n",
    "    if len(sys_data) > 0:\n",
    "        print(f\"\\n{sys_name}:\")\n",
    "        for _, row in sys_data.iterrows():\n",
    "            print(f\"  '{row['first_token']}': {row['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first token entropy by system prompt\n",
    "first_token_entropy = consistency_df.groupby('system_prompt')['first_token_entropy'].mean().sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(first_token_entropy)))\n",
    "ax.barh(range(len(first_token_entropy)), first_token_entropy.values, color=colors)\n",
    "ax.set_yticks(range(len(first_token_entropy)))\n",
    "ax.set_yticklabels(first_token_entropy.index)\n",
    "ax.set_xlabel('First Token Entropy')\n",
    "ax.set_title('First Token Variability by System Prompt\\n(Lower = More Deterministic Start)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/first_token_entropy.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SAMPLING CONSISTENCY ANALYSIS - KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. MOST CONSISTENT SYSTEM PROMPTS (highest token overlap):\")\n",
    "top_consistent = sys_consistency['token_jaccard'].nlargest(3)\n",
    "for sys_name, score in top_consistent.items():\n",
    "    print(f\"   - {sys_name}: Jaccard={score:.4f}\")\n",
    "\n",
    "print(\"\\n2. MOST VARIABLE SYSTEM PROMPTS (highest unique ratio):\")\n",
    "top_variable = sys_consistency['unique_ratio'].nlargest(3)\n",
    "for sys_name, score in top_variable.items():\n",
    "    print(f\"   - {sys_name}: Unique={score:.4f}\")\n",
    "\n",
    "print(\"\\n3. ENTROPY-CONSISTENCY CORRELATION:\")\n",
    "print(f\"   - Entropy vs Token Overlap: r={corr1:.3f}\")\n",
    "interpretation = \"Higher entropy leads to LESS consistent outputs\" if corr1 < 0 else \"Weak relationship\"\n",
    "print(f\"   - Interpretation: {interpretation}\")\n",
    "\n",
    "print(\"\\n4. CATEGORY INSIGHTS:\")\n",
    "most_consistent_cat = cat_consistency['token_jaccard'].idxmax()\n",
    "least_consistent_cat = cat_consistency['token_jaccard'].idxmin()\n",
    "print(f\"   - Most consistent category: {most_consistent_cat}\")\n",
    "print(f\"   - Most variable category: {least_consistent_cat}\")\n",
    "\n",
    "print(\"\\n5. PRACTICAL IMPLICATIONS:\")\n",
    "print(\"   - For reproducible outputs: Use system prompts with high first-token consistency\")\n",
    "print(\"   - For creative diversity: Use system prompts with high unique ratio\")\n",
    "print(\"   - Lower temperature reduces variance for all system prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "results = {\n",
    "    'system_consistency': sys_consistency.to_dict(),\n",
    "    'category_consistency': cat_consistency.to_dict(),\n",
    "    'entropy_correlations': {\n",
    "        'entropy_vs_jaccard': {'r': corr1, 'p': p1},\n",
    "        'entropy_vs_unique': {'r': corr2, 'p': p2},\n",
    "        'entropy_vs_first_token': {'r': corr3, 'p': p3}\n",
    "    },\n",
    "    'config': {\n",
    "        'n_samples': N_SAMPLES,\n",
    "        'max_new_tokens': MAX_NEW_TOKENS,\n",
    "        'top_p': TOP_P,\n",
    "        'temperature': TEMPERATURE\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../results/sampling_consistency.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=float)\n",
    "\n",
    "consistency_df.to_csv('../results/sampling_consistency_full.csv', index=False)\n",
    "samples_df.to_csv('../results/all_samples.csv', index=False)\n",
    "\n",
    "print(\"Results saved to ../results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
