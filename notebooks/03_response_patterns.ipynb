{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Response Pattern Analysis\n",
    "\n",
    "**Goal:** Analyze how system prompts change response PATTERNS (not just distributions).\n",
    "\n",
    "**Setup:**\n",
    "- Fixed test prompts\n",
    "- Generate actual responses under different system prompts\n",
    "- Analyze: length, structure, confidence markers, hedging\n",
    "\n",
    "**Key Questions:**\n",
    "- Do verbose system prompts produce verbose responses?\n",
    "- Do cautious prompts increase hedging language?\n",
    "- How do personas change response style?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup path for imports\nimport sys\nimport os\n\n# Handle both local and Colab environments\nif 'google.colab' in sys.modules:\n    # In Colab - go to repo root\n    repo_root = '/content/LLM-Instruction-Understanding'\n    if os.path.exists(repo_root):\n        os.chdir(repo_root)\n        if repo_root not in sys.path:\n            sys.path.insert(0, repo_root)\nelse:\n    # Local - add parent directory\n    parent = os.path.abspath('..')\n    if parent not in sys.path:\n        sys.path.insert(0, parent)\n\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use subset of system prompts for generation\n",
    "SYSTEM_SUBSET = {k: SYSTEM_PROMPTS[k] for k in [\n",
    "    \"none\", \"minimal\", \"expert\", \"concise\", \"verbose\", \"cautious\", \"confident\", \"cot\"\n",
    "]}\n",
    "\n",
    "# Use subset of test prompts\n",
    "TEST_SUBSET = ALL_TEST_PROMPTS[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, test_prompts, system_prompts, max_tokens=100):\n",
    "    results = []\n",
    "    total = len(test_prompts) * len(system_prompts)\n",
    "    pbar = tqdm(total=total, desc=\"Generating responses\")\n",
    "    \n",
    "    for test in test_prompts:\n",
    "        for sys_name, sys_info in system_prompts.items():\n",
    "            prompt = build_chat_prompt(sys_info[\"text\"], test[\"prompt\"], model.tokenizer)\n",
    "            \n",
    "            output = model.generate_with_probs(\n",
    "                prompt, max_new_tokens=max_tokens, temperature=0.7\n",
    "            )\n",
    "            \n",
    "            # GenerationOutput is a dataclass with attributes\n",
    "            results.append({\n",
    "                \"test_id\": test[\"id\"],\n",
    "                \"test_prompt\": test[\"prompt\"],\n",
    "                \"category\": test[\"category\"],\n",
    "                \"system_prompt\": sys_name,\n",
    "                \"response\": output.text,\n",
    "                \"n_tokens\": len(output.tokens),\n",
    "                \"mean_log_prob\": np.mean(output.log_probs) if output.log_probs else 0\n",
    "            })\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "responses_df = generate_responses(model, TEST_SUBSET, SYSTEM_SUBSET, max_tokens=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Response Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linguistic markers\n",
    "HEDGING_WORDS = [\"maybe\", \"perhaps\", \"might\", \"could\", \"possibly\", \"probably\", \n",
    "                 \"uncertain\", \"not sure\", \"i think\", \"i believe\", \"it seems\"]\n",
    "CONFIDENT_WORDS = [\"definitely\", \"certainly\", \"absolutely\", \"clearly\", \"obviously\",\n",
    "                   \"of course\", \"without doubt\", \"sure\"]\n",
    "REASONING_MARKERS = [\"because\", \"therefore\", \"thus\", \"since\", \"first\", \"second\",\n",
    "                     \"step\", \"reason\", \"conclude\"]\n",
    "\n",
    "def extract_features(response):\n",
    "    text = response.lower()\n",
    "    words = text.split()\n",
    "    \n",
    "    return {\n",
    "        \"length_chars\": len(response),\n",
    "        \"length_words\": len(words),\n",
    "        \"length_sentences\": len(re.split(r'[.!?]+', response)),\n",
    "        \"hedging_count\": sum(1 for h in HEDGING_WORDS if h in text),\n",
    "        \"confident_count\": sum(1 for c in CONFIDENT_WORDS if c in text),\n",
    "        \"reasoning_count\": sum(1 for r in REASONING_MARKERS if r in text),\n",
    "        \"question_marks\": response.count(\"?\"),\n",
    "        \"exclamation_marks\": response.count(\"!\"),\n",
    "        \"has_list\": 1 if re.search(r'\\d\\.\\s|\\n-\\s|\\n\\*\\s', response) else 0,\n",
    "        \"first_person\": sum(1 for w in [\"i\", \"my\", \"me\"] if f\" {w} \" in f\" {text} \"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all responses\n",
    "features = []\n",
    "for _, row in responses_df.iterrows():\n",
    "    feat = extract_features(row[\"response\"])\n",
    "    feat[\"system_prompt\"] = row[\"system_prompt\"]\n",
    "    feat[\"test_id\"] = row[\"test_id\"]\n",
    "    feat[\"category\"] = row[\"category\"]\n",
    "    features.append(feat)\n",
    "\n",
    "features_df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Patterns by System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by system prompt\n",
    "pattern_summary = features_df.groupby('system_prompt').agg({\n",
    "    'length_words': 'mean',\n",
    "    'hedging_count': 'mean',\n",
    "    'confident_count': 'mean',\n",
    "    'reasoning_count': 'mean',\n",
    "    'has_list': 'mean',\n",
    "    'first_person': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"=== Response Patterns by System Prompt ===\")\n",
    "print(pattern_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "metrics = ['length_words', 'hedging_count', 'confident_count', \n",
    "           'reasoning_count', 'has_list', 'first_person']\n",
    "titles = ['Response Length', 'Hedging Words', 'Confident Words',\n",
    "          'Reasoning Markers', 'List Format Rate', 'First Person Usage']\n",
    "\n",
    "for ax, metric, title in zip(axes.flatten(), metrics, titles):\n",
    "    data = pattern_summary[metric].sort_values()\n",
    "    ax.barh(range(len(data)), data.values, alpha=0.7)\n",
    "    ax.set_yticks(range(len(data)))\n",
    "    ax.set_yticklabels(data.index)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp3_response_patterns.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Expected Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Verification of Expected Behaviors ===\")\n",
    "\n",
    "# Concise should have shorter responses\n",
    "concise_len = pattern_summary.loc['concise', 'length_words']\n",
    "verbose_len = pattern_summary.loc['verbose', 'length_words']\n",
    "print(f\"\\n1. Concise vs Verbose length: {concise_len:.1f} vs {verbose_len:.1f} words\")\n",
    "print(f\"   \u2192 {'\u2713 Working' if concise_len < verbose_len else '\u2717 Not working'}\")\n",
    "\n",
    "# Cautious should have more hedging\n",
    "cautious_hedge = pattern_summary.loc['cautious', 'hedging_count']\n",
    "confident_hedge = pattern_summary.loc['confident', 'hedging_count']\n",
    "print(f\"\\n2. Cautious vs Confident hedging: {cautious_hedge:.2f} vs {confident_hedge:.2f}\")\n",
    "print(f\"   \u2192 {'\u2713 Working' if cautious_hedge > confident_hedge else '\u2717 Not working'}\")\n",
    "\n",
    "# CoT should have more reasoning markers\n",
    "cot_reasoning = pattern_summary.loc['cot', 'reasoning_count']\n",
    "none_reasoning = pattern_summary.loc['none', 'reasoning_count']\n",
    "print(f\"\\n3. CoT vs None reasoning markers: {cot_reasoning:.2f} vs {none_reasoning:.2f}\")\n",
    "print(f\"   \u2192 {'\u2713 Working' if cot_reasoning > none_reasoning else '\u2717 Not working'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Responses Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample responses for same prompt under different system prompts\n",
    "sample_test = TEST_SUBSET[0]\n",
    "\n",
    "print(f\"Test Prompt: {sample_test['prompt']}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for sys_name in ['none', 'concise', 'verbose', 'cot']:\n",
    "    response = responses_df[\n",
    "        (responses_df['test_id'] == sample_test['id']) & \n",
    "        (responses_df['system_prompt'] == sys_name)\n",
    "    ]['response'].values[0]\n",
    "    \n",
    "    print(f\"\\n[{sys_name.upper()}]\")\n",
    "    print(response[:300] + \"...\" if len(response) > 300 else response)\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "import json\n",
    "with open('../results/exp3_results.json', 'w') as f:\n",
    "    json.dump({\"pattern_summary\": pattern_summary.to_dict()}, f, indent=2, default=float)\n",
    "print(\"Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}