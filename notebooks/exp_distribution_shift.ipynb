{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Distribution Shift Analysis\n",
    "\n",
    "**Goal:** Measure how different system prompts change the output distribution for the SAME user queries.\n",
    "\n",
    "**Metrics:** KL divergence, JS divergence, entropy, top-k overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    if not os.path.exists('/content/LLM-Instruction-Understanding'):\n",
    "        !git clone https://github.com/maralkh/LLM-Instruction-Understanding.git\n",
    "    os.chdir('/content/LLM-Instruction-Understanding')\n",
    "    !pip install -q -r requirements.txt\n",
    "    sys.path.insert(0, '/content/LLM-Instruction-Understanding')\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.metrics import DistributionMetrics, compute_all_metrics\n",
    "from src.test_configs import get_all_test_prompts, get_system_prompts, build_chat_prompt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "test_prompts = get_all_test_prompts()\n",
    "system_prompts = get_system_prompts()\n",
    "\n",
    "print(f\"Testing {len(test_prompts)} prompts × {len(system_prompts)} system prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_distributions(model, test_prompts, system_prompts, max_tests=20):\n",
    "    \"\"\"Collect output distributions for all combinations.\"\"\"\n",
    "    results = []\n",
    "    test_subset = test_prompts[:max_tests]\n",
    "    \n",
    "    for test in tqdm(test_subset, desc=\"Collecting\"):\n",
    "        for sys_name, sys_info in system_prompts.items():\n",
    "            full_prompt = build_chat_prompt(sys_info['text'], test['prompt'], model.tokenizer)\n",
    "            dist = model.get_next_token_distribution(full_prompt, top_k=100)\n",
    "            \n",
    "            # Extract token and prob separately from tuples\n",
    "            top_tokens = [t[0] for t in dist['top_tokens']]\n",
    "            top_probs = [t[1] for t in dist['top_tokens']]\n",
    "            \n",
    "            results.append({\n",
    "                'test_id': test['id'],\n",
    "                'category': test['category'],\n",
    "                'system_prompt': sys_name,\n",
    "                'entropy': dist['entropy'],\n",
    "                'top_token': top_tokens[0] if top_tokens else '',\n",
    "                'top_prob': top_probs[0] if top_probs else 0,\n",
    "                'top_5_tokens': top_tokens[:5],\n",
    "                'full_probs': dist['full_probs']\n",
    "            })\n",
    "    return results\n",
    "\n",
    "all_distributions = collect_distributions(model, test_prompts, system_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Divergences from Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_divergences(distributions, baseline='none'):\n",
    "    \"\"\"Calculate divergence from baseline for each system prompt.\"\"\"\n",
    "    # Group by test_id\n",
    "    by_test = {}\n",
    "    for d in distributions:\n",
    "        tid = d['test_id']\n",
    "        if tid not in by_test:\n",
    "            by_test[tid] = {}\n",
    "        by_test[tid][d['system_prompt']] = d\n",
    "    \n",
    "    divergences = []\n",
    "    for test_id, sys_dists in by_test.items():\n",
    "        if baseline not in sys_dists:\n",
    "            continue\n",
    "        base = sys_dists[baseline]\n",
    "        \n",
    "        for sys_name, sys_dist in sys_dists.items():\n",
    "            if sys_name == baseline:\n",
    "                continue\n",
    "            \n",
    "            # Use improved metrics with numerical stability\n",
    "            js = DistributionMetrics.jensen_shannon(base['full_probs'], sys_dist['full_probs'])\n",
    "            kl = DistributionMetrics.kl_divergence(base['full_probs'], sys_dist['full_probs'])\n",
    "            overlap = DistributionMetrics.top_k_overlap(base['top_5_tokens'], sys_dist['top_5_tokens'], k=5)\n",
    "            \n",
    "            divergences.append({\n",
    "                'test_id': test_id,\n",
    "                'category': base['category'],\n",
    "                'system_prompt': sys_name,\n",
    "                'js_divergence': js,\n",
    "                'kl_divergence': kl,\n",
    "                'top_5_overlap': overlap,\n",
    "                'entropy_change': sys_dist['entropy'] - base['entropy'],\n",
    "                'top_token_changed': base['top_token'] != sys_dist['top_token']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(divergences)\n",
    "\n",
    "divergence_df = calculate_divergences(all_distributions)\n",
    "print(f\"Calculated {len(divergence_df)} divergences\")\n",
    "print(f\"\\nNaN check: JS={divergence_df['js_divergence'].isna().sum()}, KL={divergence_df['kl_divergence'].isna().sum()}\")\n",
    "print(f\"\\nJS stats: min={divergence_df['js_divergence'].min():.6f}, max={divergence_df['js_divergence'].max():.6f}, mean={divergence_df['js_divergence'].mean():.6f}\")\n",
    "print(f\"KL stats: min={divergence_df['kl_divergence'].min():.6f}, max={divergence_df['kl_divergence'].max():.6f}, mean={divergence_df['kl_divergence'].mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Prompt Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any invalid values\n",
    "valid_df = divergence_df[divergence_df['js_divergence'].apply(lambda x: np.isfinite(x))]\n",
    "\n",
    "sys_impact = valid_df.groupby('system_prompt').agg({\n",
    "    'js_divergence': ['mean', 'std'],\n",
    "    'entropy_change': 'mean',\n",
    "    'top_token_changed': 'mean'\n",
    "}).round(4)\n",
    "sys_impact.columns = ['js_mean', 'js_std', 'entropy_change', 'top_change_rate']\n",
    "sys_impact = sys_impact.sort_values('js_mean', ascending=False)\n",
    "\n",
    "print(\"=== System Prompt Impact ===\")\n",
    "print(sys_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "s = sys_impact.sort_values('js_mean')\n",
    "\n",
    "# JS Divergence\n",
    "ax = axes[0]\n",
    "ax.barh(range(len(s)), s['js_mean'], xerr=s['js_std'], capsize=3, alpha=0.7)\n",
    "ax.set_yticks(range(len(s)))\n",
    "ax.set_yticklabels(s.index)\n",
    "ax.set_xlabel('JS Divergence')\n",
    "ax.set_title('Distribution Shift from Baseline')\n",
    "\n",
    "# Entropy change\n",
    "ax = axes[1]\n",
    "colors = ['green' if x > 0 else 'red' for x in s['entropy_change']]\n",
    "ax.barh(range(len(s)), s['entropy_change'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(s)))\n",
    "ax.set_yticklabels(s.index)\n",
    "ax.set_xlabel('Entropy Change')\n",
    "ax.set_title('Uncertainty Change\\n(+ve = more uncertain)')\n",
    "\n",
    "# Top token change rate\n",
    "ax = axes[2]\n",
    "ax.barh(range(len(s)), s['top_change_rate'] * 100, alpha=0.7)\n",
    "ax.set_yticks(range(len(s)))\n",
    "ax.set_yticklabels(s.index)\n",
    "ax.set_xlabel('% Top Token Changed')\n",
    "ax.set_title('How Often Top Prediction Changes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/distribution_shift.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Category Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: System Prompt × Category\n",
    "pivot = valid_df.pivot_table(values='js_divergence', index='system_prompt', columns='category', aggfunc='mean')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax)\n",
    "ax.set_title('JS Divergence: System Prompt × Category')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/distribution_heatmap.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DISTRIBUTION SHIFT ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. HIGHEST IMPACT System Prompts:\")\n",
    "for sys in sys_impact.head(3).index:\n",
    "    print(f\"   • {sys}: JS={sys_impact.loc[sys, 'js_mean']:.4f}\")\n",
    "\n",
    "print(\"\\n2. LOWEST IMPACT System Prompts:\")\n",
    "for sys in sys_impact.tail(3).index:\n",
    "    print(f\"   • {sys}: JS={sys_impact.loc[sys, 'js_mean']:.4f}\")\n",
    "\n",
    "# Save\n",
    "import json\n",
    "with open('../results/distribution_shift.json', 'w') as f:\n",
    "    json.dump({'system_impact': sys_impact.to_dict()}, f, indent=2, default=float)\n",
    "print(\"\\nResults saved.\")"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
