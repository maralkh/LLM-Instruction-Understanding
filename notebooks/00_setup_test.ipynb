{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Test Notebook\n",
    "\n",
    "Run this first to verify everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLM-Instruction-Understanding'...\n",
      "remote: Enumerating objects: 52, done.\u001b[K\n",
      "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
      "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
      "remote: Total 52 (delta 10), reused 48 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (52/52), 89.99 KiB | 12.86 MiB/s, done.\n",
      "Resolving deltas: 100% (10/10), done.\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hWorking directory: /content/LLM-Instruction-Understanding\n"
     ]
    }
   ],
   "source": [
    "# For Google Colab: Clone repo and install dependencies\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone if not exists\n",
    "    if not os.path.exists('/content/LLM-Instruction-Understanding'):\n",
    "        !git clone https://github.com/maralkh/LLM-Instruction-Understanding.git\n",
    "    \n",
    "    # Change to repo directory\n",
    "    os.chdir('/content/LLM-Instruction-Understanding')\n",
    "    \n",
    "    # Install requirements\n",
    "    !pip install -q -r requirements.txt\n",
    "    \n",
    "    # Add to path\n",
    "    if '/content/LLM-Instruction-Understanding' not in sys.path:\n",
    "        sys.path.insert(0, '/content/LLM-Instruction-Understanding')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing imports...\n",
      "✓ model_utils\n",
      "✓ metrics\n",
      "✓ visualization\n",
      "✓ test_configs\n",
      "\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Test imports\n",
    "print(\"Testing imports...\")\n",
    "\n",
    "try:\n",
    "    from src.model_utils import load_model\n",
    "    print(\"✓ model_utils\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ model_utils: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.metrics import DistributionMetrics\n",
    "    print(\"✓ metrics\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ metrics: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.visualization import set_style\n",
    "    print(\"✓ visualization\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ visualization: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.test_configs import TEST_PROMPTS, SYSTEM_PROMPTS, build_chat_prompt\n",
    "    print(\"✓ test_configs\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ test_configs: {e}\")\n",
    "\n",
    "print(\"\\nAll imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompt categories: ['factual', 'reasoning', 'classification', 'creative', 'instruction_following', 'edge_cases']\n",
      "Total test prompts: 30\n",
      "System prompts: ['none', 'minimal', 'helpful_detailed', 'expert', 'concise', 'verbose', 'cot', 'cautious', 'confident', 'friendly', 'formal', 'teacher', 'scientist', 'structured', 'safety', 'creative']\n"
     ]
    }
   ],
   "source": [
    "# Show what's available\n",
    "from src.test_configs import TEST_PROMPTS, SYSTEM_PROMPTS, ALL_TEST_PROMPTS\n",
    "\n",
    "print(f\"Test prompt categories: {list(TEST_PROMPTS.keys())}\")\n",
    "print(f\"Total test prompts: {len(ALL_TEST_PROMPTS)}\")\n",
    "print(f\"System prompts: {list(SYSTEM_PROMPTS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model (this may take a minute)...\n",
      "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9904193226b647949067cdce60d967f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fd494675ef4fc7a856d5ccea58642a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4c12d60f4f4f26a145f238f4628559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd141a32bb343618c986a9caad6e2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59e6253a2164174b79c4509ca1019c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3064178dfd4ddf988d623e7bf5529c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3995d8e709914fdb849c24dc35912409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "✓ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test model loading (this will download the model)\n",
    "print(\"Loading model (this may take a minute)...\")\n",
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompt: What is 2 + 2?\n",
      "Top 5 tokens: [('2', 0.73583984375), ('The', 0.07635498046875), ('Yes', 0.06134033203125), ('S', 0.0167694091796875), ('Answer', 0.01161956787109375)]\n",
      "Entropy: nan\n",
      "\n",
      "✓ Everything is working!\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "from src.test_configs import build_chat_prompt\n",
    "\n",
    "test_prompt = \"What is 2 + 2?\"\n",
    "system = \"You are a helpful assistant.\"\n",
    "\n",
    "full_prompt = build_chat_prompt(system, test_prompt, model.tokenizer)\n",
    "dist = model.get_next_token_distribution(full_prompt, top_k=5)\n",
    "\n",
    "print(f\"Test prompt: {test_prompt}\")\n",
    "print(f\"Top 5 tokens: {dist['top_tokens']}\")\n",
    "print(f\"Entropy: {dist['entropy']:.4f}\")\n",
    "print(\"\\n✓ Everything is working!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
