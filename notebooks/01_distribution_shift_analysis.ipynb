{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Distribution Shift Analysis\n",
    "\n",
    "**Goal:** Quantify how prompt variations change the output probability distribution.\n",
    "\n",
    "**Key Questions:**\n",
    "- How much does the next-token distribution change with different prompts?\n",
    "- Which prompt modifications cause the largest distribution shifts?\n",
    "- Is there a correlation between distribution change and task performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.model_utils import load_model, ModelConfig\n",
    "from src.prompt_utils import PromptVariantGenerator, INSTRUCTION_SPECIFICITY, FORMATTING_STYLES, PERSONAS, THINKING_STYLES\n",
    "from src.metrics import DistributionMetrics, ExperimentResults, compute_all_metrics\n",
    "from src.visualization import set_style, plot_distribution_comparison, plot_entropy_comparison, plot_dimension_heatmap\n",
    "\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model\n",
    "\n",
    "We'll use TinyLlama-1.1B as our base model. It's small enough for single-GPU experiments while being capable enough to show meaningful prompt effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = load_model(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device=\"cuda\"  # or \"cpu\" if no GPU\n",
    ")\n",
    "print(f\"Model loaded on {model.config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Test Questions\n",
    "\n",
    "We'll test across different question types to see if prompt effects are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions covering different task types\n",
    "TEST_QUESTIONS = {\n",
    "    \"factual\": \"What is the capital of France?\",\n",
    "    \"reasoning\": \"If a train travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\",\n",
    "    \"classification\": \"Is the following sentence positive or negative: 'I absolutely loved this movie!'\",\n",
    "    \"open_ended\": \"What are the main benefits of renewable energy?\",\n",
    "    \"coding\": \"Write a Python function to calculate the factorial of a number.\"\n",
    "}\n",
    "\n",
    "# Expected completions for measuring probability mass\n",
    "EXPECTED_COMPLETIONS = {\n",
    "    \"factual\": \"Paris\",\n",
    "    \"reasoning\": \"150\",\n",
    "    \"classification\": \"positive\",\n",
    "    \"open_ended\": None,  # Open-ended, no single correct answer\n",
    "    \"coding\": \"def\"  # Check if it starts with function definition\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Prompt Variants\n",
    "\n",
    "We'll vary prompts along multiple dimensions to understand what causes distribution shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate variants for one question first\n",
    "question = TEST_QUESTIONS[\"factual\"]\n",
    "\n",
    "# Vary across all dimensions\n",
    "variants = PromptVariantGenerator.create_variants(\n",
    "    question=question,\n",
    "    dimensions=['specificity', 'format', 'persona']\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(variants)} prompt variants\")\n",
    "print(\"\\nExample variants:\")\n",
    "for v in variants[:3]:\n",
    "    print(f\"\\nConfig: {v['config']}\")\n",
    "    print(f\"Prompt: {v['prompt'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Measure Distribution Shifts\n",
    "\n",
    "For each prompt variant, we'll:\n",
    "1. Get the next-token probability distribution\n",
    "2. Calculate entropy\n",
    "3. Compare to a baseline (no-frills prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prompt_variants(model, question, variants, expected_completion=None):\n",
    "    \"\"\"Analyze distribution changes across prompt variants.\"\"\"\n",
    "    results = ExperimentResults()\n",
    "    \n",
    "    # Get baseline distribution (raw question)\n",
    "    baseline_dist = model.get_next_token_distribution(question)\n",
    "    baseline_probs = baseline_dist['full_probs']\n",
    "    \n",
    "    for variant in tqdm(variants, desc=\"Analyzing variants\"):\n",
    "        prompt = variant['prompt']\n",
    "        config = variant['config']\n",
    "        \n",
    "        # Get distribution for this variant\n",
    "        dist = model.get_next_token_distribution(prompt)\n",
    "        variant_probs = dist['full_probs']\n",
    "        \n",
    "        # Calculate comparison metrics\n",
    "        metrics = compute_all_metrics(\n",
    "            baseline_probs, variant_probs,\n",
    "            baseline_dist['top_tokens'], dist['top_tokens']\n",
    "        )\n",
    "        \n",
    "        # If we have expected completion, check probability\n",
    "        if expected_completion:\n",
    "            seq_probs = model.get_sequence_log_probs(prompt, \" \" + expected_completion)\n",
    "            metrics['target_log_prob'] = seq_probs['total_log_prob']\n",
    "            metrics['target_avg_log_prob'] = seq_probs['avg_log_prob']\n",
    "        \n",
    "        results.add_result({\n",
    "            'prompt': prompt,\n",
    "            'config': config,\n",
    "            'top_5_tokens': dist['top_tokens'][:5],\n",
    "            **metrics\n",
    "        })\n",
    "    \n",
    "    return results, baseline_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on factual question\n",
    "results, baseline = analyze_prompt_variants(\n",
    "    model, \n",
    "    TEST_QUESTIONS[\"factual\"],\n",
    "    variants,\n",
    "    expected_completion=EXPECTED_COMPLETIONS[\"factual\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nAnalyzed {len(results.results)} variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = results.to_dataframe()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=== Key Metrics Summary ===\")\n",
    "for metric in ['kl_divergence', 'jensen_shannon', 'variant_entropy', 'target_log_prob']:\n",
    "    if metric in df.columns:\n",
    "        stats = results.summary_statistics(metric)\n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"  Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "        print(f\"  Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which dimension causes largest distribution shifts?\n",
    "print(\"\\n=== Distribution Shift by Dimension ===\")\n",
    "\n",
    "for dim in ['config_specificity', 'config_format', 'config_persona']:\n",
    "    if dim in df.columns:\n",
    "        grouped = df.groupby(dim)['kl_divergence'].mean().sort_values(ascending=False)\n",
    "        print(f\"\\n{dim}:\")\n",
    "        for val, kl in grouped.items():\n",
    "            print(f\"  {val}: KL={kl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Top configurations by target probability\n",
    "if 'target_log_prob' in df.columns:\n",
    "    top_10 = df.nlargest(10, 'target_log_prob')\n",
    "    bottom_10 = df.nsmallest(10, 'target_log_prob')\n",
    "    \n",
    "    print(\"\\n=== Top 10 Prompts for Target Completion ===\")\n",
    "    for _, row in top_10.iterrows():\n",
    "        config = {k.replace('config_', ''): row[k] for k in row.index if k.startswith('config_')}\n",
    "        print(f\"Log-prob: {row['target_log_prob']:.3f} | Config: {config}\")\n",
    "    \n",
    "    print(\"\\n=== Bottom 10 Prompts for Target Completion ===\")\n",
    "    for _, row in bottom_10.iterrows():\n",
    "        config = {k.replace('config_', ''): row[k] for k in row.index if k.startswith('config_')}\n",
    "        print(f\"Log-prob: {row['target_log_prob']:.3f} | Config: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions for best vs worst prompts\n",
    "if 'target_log_prob' in df.columns:\n",
    "    best_idx = df['target_log_prob'].idxmax()\n",
    "    worst_idx = df['target_log_prob'].idxmin()\n",
    "    \n",
    "    best_result = results.results[best_idx]\n",
    "    worst_result = results.results[worst_idx]\n",
    "    \n",
    "    # Get distributions\n",
    "    best_dist = model.get_next_token_distribution(best_result['prompt'])\n",
    "    worst_dist = model.get_next_token_distribution(worst_result['prompt'])\n",
    "    \n",
    "    fig = plot_distribution_comparison(\n",
    "        [baseline, best_dist, worst_dist],\n",
    "        ['Baseline (raw question)', 'Best Prompt', 'Worst Prompt'],\n",
    "        title=f'Distribution Comparison for: \"{TEST_QUESTIONS[\"factual\"]}\"'\n",
    "    )\n",
    "    plt.savefig('../results/exp1_distribution_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Specificity × Format\n",
    "if 'config_specificity' in df.columns and 'config_format' in df.columns:\n",
    "    fig = plot_dimension_heatmap(\n",
    "        df, 'config_specificity', 'config_format', 'kl_divergence',\n",
    "        title='KL Divergence from Baseline: Specificity × Format'\n",
    "    )\n",
    "    plt.savefig('../results/exp1_heatmap_specificity_format.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy by configuration\n",
    "fig = plot_entropy_comparison(\n",
    "    results.results[:20],  # Top 20 for readability\n",
    "    title='Next-Token Entropy Across Prompt Variants'\n",
    ")\n",
    "plt.savefig('../results/exp1_entropy_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Question Analysis\n",
    "\n",
    "Let's see if the same prompt variations have consistent effects across different questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze across all question types\n",
    "all_results = {}\n",
    "\n",
    "for q_type, question in TEST_QUESTIONS.items():\n",
    "    print(f\"\\nAnalyzing: {q_type}\")\n",
    "    \n",
    "    variants = PromptVariantGenerator.create_variants(\n",
    "        question=question,\n",
    "        dimensions=['specificity', 'format']\n",
    "    )\n",
    "    \n",
    "    results_q, _ = analyze_prompt_variants(\n",
    "        model, question, variants,\n",
    "        expected_completion=EXPECTED_COMPLETIONS.get(q_type)\n",
    "    )\n",
    "    \n",
    "    all_results[q_type] = results_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare which configurations work best across question types\n",
    "print(\"\\n=== Best Configuration by Question Type ===\")\n",
    "\n",
    "for q_type, results_q in all_results.items():\n",
    "    df_q = results_q.to_dataframe()\n",
    "    \n",
    "    # Best by lowest entropy (more confident)\n",
    "    best_entropy = df_q.loc[df_q['variant_entropy'].idxmin()]\n",
    "    config = {k.replace('config_', ''): best_entropy[k] for k in best_entropy.index if k.startswith('config_')}\n",
    "    print(f\"\\n{q_type} - Lowest Entropy: {best_entropy['variant_entropy']:.3f}\")\n",
    "    print(f\"  Config: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings\n",
    "\n",
    "Summarize observations from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate findings\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 1 SUMMARY: Distribution Shift Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate aggregate statistics across all question types\n",
    "all_kl = []\n",
    "all_entropy_changes = []\n",
    "\n",
    "for q_type, results_q in all_results.items():\n",
    "    df_q = results_q.to_dataframe()\n",
    "    all_kl.extend(df_q['kl_divergence'].tolist())\n",
    "    all_entropy_changes.extend((df_q['variant_entropy'] - df_q['baseline_entropy']).tolist())\n",
    "\n",
    "print(f\"\\n1. Distribution Shift Magnitude:\")\n",
    "print(f\"   - Average KL divergence from baseline: {np.mean(all_kl):.4f}\")\n",
    "print(f\"   - Max KL divergence observed: {np.max(all_kl):.4f}\")\n",
    "\n",
    "print(f\"\\n2. Entropy Changes:\")\n",
    "print(f\"   - Average entropy change: {np.mean(all_entropy_changes):.4f}\")\n",
    "print(f\"   - Prompts that increase entropy: {sum(1 for e in all_entropy_changes if e > 0)}\")\n",
    "print(f\"   - Prompts that decrease entropy: {sum(1 for e in all_entropy_changes if e < 0)}\")\n",
    "\n",
    "print(f\"\\n3. Key Observations:\")\n",
    "print(f\"   - [Fill in after running experiments]\")\n",
    "print(f\"   - [Which dimensions matter most?]\")\n",
    "print(f\"   - [Are effects consistent across question types?]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "for q_type, results_q in all_results.items():\n",
    "    results_q.save(f'../results/exp1_results_{q_type}.json')\n",
    "\n",
    "print(\"Results saved to ../results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
