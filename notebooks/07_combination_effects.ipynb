{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7: System Prompt Combination Effects\n",
    "\n",
    "**Goal:** Test how combining multiple instructions in system prompts affects behavior.\n",
    "\n",
    "**Setup:**\n",
    "- Fixed test prompts\n",
    "- Individual instructions vs combinations\n",
    "- Measure: Do effects stack? Conflict? Cancel out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup path for imports\nimport sys\nimport os\n\n# Handle both local and Colab environments\nif 'google.colab' in sys.modules:\n    # In Colab - go to repo root\n    repo_root = '/content/LLM-Instruction-Understanding'\n    if os.path.exists(repo_root):\n        os.chdir(repo_root)\n        if repo_root not in sys.path:\n            sys.path.insert(0, repo_root)\nelse:\n    # Local - add parent directory\n    parent = os.path.abspath('..')\n    if parent not in sys.path:\n        sys.path.insert(0, parent)\n\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Individual Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDIVIDUAL_INSTRUCTIONS = {\n",
    "    \"concise\": \"Be concise.\",\n",
    "    \"accurate\": \"Be accurate.\",\n",
    "    \"helpful\": \"Be helpful.\",\n",
    "    \"cot\": \"Think step by step.\",\n",
    "    \"confident\": \"Be confident.\",\n",
    "    \"cautious\": \"Be cautious about uncertainty.\",\n",
    "}\n",
    "\n",
    "# Potentially conflicting pairs\n",
    "CONFLICT_PAIRS = [\n",
    "    (\"concise\", \"cot\"),  # Short vs detailed reasoning\n",
    "    (\"confident\", \"cautious\"),  # Opposite attitudes\n",
    "]\n",
    "\n",
    "# Potentially synergistic pairs\n",
    "SYNERGY_PAIRS = [\n",
    "    (\"accurate\", \"cautious\"),\n",
    "    (\"helpful\", \"cot\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Individual vs Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUBSET = ALL_TEST_PROMPTS[:10]\n",
    "\n",
    "def test_combinations(model, test_prompts, instructions):\n",
    "    results = []\n",
    "    \n",
    "    # Baseline (no instruction)\n",
    "    for test in test_prompts:\n",
    "        prompt = build_chat_prompt(\"\", test[\"prompt\"], model.tokenizer)\n",
    "        dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "        results.append({\n",
    "            \"config\": \"none\",\n",
    "            \"test_id\": test[\"id\"],\n",
    "            \"entropy\": dist[\"entropy\"],\n",
    "            \"full_probs\": dist[\"full_probs\"]\n",
    "        })\n",
    "    \n",
    "    # Individual instructions\n",
    "    for name, text in tqdm(instructions.items(), desc=\"Individual\"):\n",
    "        for test in test_prompts:\n",
    "            prompt = build_chat_prompt(text, test[\"prompt\"], model.tokenizer)\n",
    "            dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "            results.append({\n",
    "                \"config\": name,\n",
    "                \"test_id\": test[\"id\"],\n",
    "                \"entropy\": dist[\"entropy\"],\n",
    "                \"full_probs\": dist[\"full_probs\"]\n",
    "            })\n",
    "    \n",
    "    # All pairs\n",
    "    for name1, name2 in tqdm(list(combinations(instructions.keys(), 2)), desc=\"Pairs\"):\n",
    "        combined = f\"{instructions[name1]} {instructions[name2]}\"\n",
    "        for test in test_prompts:\n",
    "            prompt = build_chat_prompt(combined, test[\"prompt\"], model.tokenizer)\n",
    "            dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "            results.append({\n",
    "                \"config\": f\"{name1}+{name2}\",\n",
    "                \"test_id\": test[\"id\"],\n",
    "                \"entropy\": dist[\"entropy\"],\n",
    "                \"full_probs\": dist[\"full_probs\"]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "combo_results = test_combinations(model, TEST_SUBSET, INDIVIDUAL_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Combination Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "combo_df = pd.DataFrame(combo_results)\n",
    "\n",
    "# Get baseline distributions per test\n",
    "baseline_by_test = {r[\"test_id\"]: r[\"full_probs\"] \n",
    "                    for r in combo_results if r[\"config\"] == \"none\"}\n",
    "\n",
    "# Calculate JS from baseline for each config\n",
    "js_from_baseline = []\n",
    "for _, row in combo_df.iterrows():\n",
    "    if row[\"config\"] != \"none\":\n",
    "        js = DistributionMetrics.jensen_shannon(\n",
    "            baseline_by_test[row[\"test_id\"]], row[\"full_probs\"]\n",
    "        )\n",
    "        js_from_baseline.append({\n",
    "            \"config\": row[\"config\"],\n",
    "            \"test_id\": row[\"test_id\"],\n",
    "            \"js_from_baseline\": js,\n",
    "            \"entropy\": row[\"entropy\"]\n",
    "        })\n",
    "\n",
    "js_df = pd.DataFrame(js_from_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by config\n",
    "config_effects = js_df.groupby('config').agg({\n",
    "    'js_from_baseline': ['mean', 'std'],\n",
    "    'entropy': 'mean'\n",
    "}).round(4)\n",
    "config_effects.columns = ['js_mean', 'js_std', 'entropy']\n",
    "\n",
    "# Separate individual vs pairs\n",
    "individual_configs = [c for c in config_effects.index if '+' not in c]\n",
    "pair_configs = [c for c in config_effects.index if '+' in c]\n",
    "\n",
    "print(\"=== Individual Instruction Effects ===\")\n",
    "print(config_effects.loc[individual_configs].sort_values('js_mean', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for interaction effects\n",
    "def get_interaction_effect(config_effects, inst1, inst2):\n",
    "    \"\"\"Compare combined effect vs sum of individual effects.\"\"\"\n",
    "    ind1 = config_effects.loc[inst1, 'js_mean']\n",
    "    ind2 = config_effects.loc[inst2, 'js_mean']\n",
    "    combined_key = f\"{inst1}+{inst2}\" if f\"{inst1}+{inst2}\" in config_effects.index else f\"{inst2}+{inst1}\"\n",
    "    combined = config_effects.loc[combined_key, 'js_mean']\n",
    "    \n",
    "    expected = (ind1 + ind2) / 2  # Simple average as baseline\n",
    "    interaction = combined - expected\n",
    "    \n",
    "    return {\n",
    "        \"pair\": f\"{inst1}+{inst2}\",\n",
    "        \"individual_1\": ind1,\n",
    "        \"individual_2\": ind2,\n",
    "        \"expected\": expected,\n",
    "        \"combined\": combined,\n",
    "        \"interaction\": interaction,\n",
    "        \"interaction_type\": \"synergy\" if interaction > 0.01 else \"conflict\" if interaction < -0.01 else \"additive\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interaction effects for all pairs\n",
    "interactions = []\n",
    "for inst1, inst2 in combinations(INDIVIDUAL_INSTRUCTIONS.keys(), 2):\n",
    "    try:\n",
    "        effect = get_interaction_effect(config_effects, inst1, inst2)\n",
    "        interactions.append(effect)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "interaction_df = pd.DataFrame(interactions).sort_values('interaction', ascending=False)\n",
    "\n",
    "print(\"=== Interaction Effects ===\")\n",
    "print(\"Positive = synergy (combined > expected)\")\n",
    "print(\"Negative = conflict (combined < expected)\")\n",
    "print(interaction_df[['pair', 'expected', 'combined', 'interaction', 'interaction_type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual effects\n",
    "ax = axes[0]\n",
    "ind_data = config_effects.loc[individual_configs].sort_values('js_mean')\n",
    "ax.barh(range(len(ind_data)), ind_data['js_mean'], xerr=ind_data['js_std'], capsize=3, alpha=0.7)\n",
    "ax.set_yticks(range(len(ind_data)))\n",
    "ax.set_yticklabels(ind_data.index)\n",
    "ax.set_xlabel('JS Divergence from Baseline')\n",
    "ax.set_title('Individual Instruction Effects')\n",
    "\n",
    "# Interaction effects\n",
    "ax = axes[1]\n",
    "colors = ['green' if x == 'synergy' else 'red' if x == 'conflict' else 'gray' \n",
    "          for x in interaction_df['interaction_type']]\n",
    "ax.barh(range(len(interaction_df)), interaction_df['interaction'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(interaction_df)))\n",
    "ax.set_yticklabels(interaction_df['pair'])\n",
    "ax.set_xlabel('Interaction Effect')\n",
    "ax.set_title('Combination Interactions\\n(Green=Synergy, Red=Conflict)')\n",
    "ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp7_combinations.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Key Findings ===\")\n",
    "synergies = interaction_df[interaction_df['interaction_type'] == 'synergy']\n",
    "conflicts = interaction_df[interaction_df['interaction_type'] == 'conflict']\n",
    "\n",
    "print(f\"Synergistic pairs: {len(synergies)}\")\n",
    "for _, row in synergies.iterrows():\n",
    "    print(f\"  \u2022 {row['pair']}\")\n",
    "\n",
    "print(f\"\\nConflicting pairs: {len(conflicts)}\")\n",
    "for _, row in conflicts.iterrows():\n",
    "    print(f\"  \u2022 {row['pair']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../results/exp7_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"individual_effects\": config_effects.loc[individual_configs].to_dict(),\n",
    "        \"interactions\": interaction_df.to_dict('records')\n",
    "    }, f, indent=2, default=float)\n",
    "print(\"Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}