{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8: Attention Pattern Analysis\n",
    "\n",
    "**Goal:** Understand where the model \"looks\" under different prompts.\n",
    "\n",
    "**Key Questions:**\n",
    "- Does \"think step by step\" change attention to intermediate tokens?\n",
    "- Do few-shot examples create attention shortcuts?\n",
    "- Which layers show the most prompt-sensitivity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.visualization import set_style\n",
    "\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention Extraction Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_patterns(model, prompt, return_tokens=True):\n",
    "    \"\"\"\n",
    "    Extract attention patterns from all layers and heads.\n",
    "    \n",
    "    Returns:\n",
    "        attentions: [n_layers, n_heads, seq_len, seq_len]\n",
    "        tokens: list of token strings\n",
    "    \"\"\"\n",
    "    inputs = model.tokenizer(prompt, return_tensors=\"pt\").to(model.config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.model(\n",
    "            **inputs,\n",
    "            output_attentions=True\n",
    "        )\n",
    "    \n",
    "    # Stack attention from all layers: [n_layers, batch, n_heads, seq_len, seq_len]\n",
    "    attentions = torch.stack(outputs.attentions).squeeze(1)  # Remove batch dim\n",
    "    attentions = attentions.cpu().numpy()\n",
    "    \n",
    "    if return_tokens:\n",
    "        tokens = [model.tokenizer.decode([t]) for t in inputs.input_ids[0]]\n",
    "        return attentions, tokens\n",
    "    \n",
    "    return attentions\n",
    "\n",
    "\n",
    "def aggregate_attention(attentions, method='mean'):\n",
    "    \"\"\"\n",
    "    Aggregate attention across layers and heads.\n",
    "    \n",
    "    Args:\n",
    "        attentions: [n_layers, n_heads, seq_len, seq_len]\n",
    "        method: 'mean', 'max', or 'last_layer'\n",
    "    \"\"\"\n",
    "    if method == 'mean':\n",
    "        return attentions.mean(axis=(0, 1))  # [seq_len, seq_len]\n",
    "    elif method == 'max':\n",
    "        return attentions.max(axis=(0, 1))\n",
    "    elif method == 'last_layer':\n",
    "        return attentions[-1].mean(axis=0)  # Last layer, mean across heads\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Attention Patterns Across Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts with same question but different instructions\n",
    "BASE_QUESTION = \"What is 15 plus 27?\"\n",
    "\n",
    "PROMPT_VARIANTS = {\n",
    "    \"plain\": BASE_QUESTION,\n",
    "    \n",
    "    \"cot\": f\"\"\"Let's think step by step.\n",
    "\n",
    "{BASE_QUESTION}\"\"\",\n",
    "    \n",
    "    \"expert\": f\"\"\"You are an expert mathematician.\n",
    "\n",
    "{BASE_QUESTION}\"\"\",\n",
    "    \n",
    "    \"structured\": f\"\"\"Question: {BASE_QUESTION}\n",
    "Answer:\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention for each variant\n",
    "attention_data = {}\n",
    "\n",
    "for name, prompt in PROMPT_VARIANTS.items():\n",
    "    print(f\"\\nExtracting attention for: {name}\")\n",
    "    attentions, tokens = get_attention_patterns(model, prompt)\n",
    "    \n",
    "    attention_data[name] = {\n",
    "        \"attentions\": attentions,\n",
    "        \"tokens\": tokens,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    \n",
    "    print(f\"  Shape: {attentions.shape}\")\n",
    "    print(f\"  Tokens: {tokens[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, data) in enumerate(attention_data.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Aggregate attention\n",
    "    agg_attn = aggregate_attention(data[\"attentions\"], method='last_layer')\n",
    "    \n",
    "    # Plot heatmap\n",
    "    tokens = data[\"tokens\"]\n",
    "    # Truncate for visibility\n",
    "    max_tokens = min(20, len(tokens))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        agg_attn[:max_tokens, :max_tokens],\n",
    "        xticklabels=[t[:8] for t in tokens[:max_tokens]],\n",
    "        yticklabels=[t[:8] for t in tokens[:max_tokens]],\n",
    "        ax=ax,\n",
    "        cmap='Blues',\n",
    "        cbar_kws={'shrink': 0.5}\n",
    "    )\n",
    "    ax.set_title(f'{name}\\n(Last Layer, Mean Heads)')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp8_attention_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention to Key Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_to_keywords(attentions, tokens, keywords):\n",
    "    \"\"\"\n",
    "    Compute how much attention the last token pays to specific keywords.\n",
    "    \"\"\"\n",
    "    # Find keyword positions\n",
    "    keyword_positions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        for kw in keywords:\n",
    "            if kw.lower() in token.lower():\n",
    "                keyword_positions.append((i, token, kw))\n",
    "    \n",
    "    if not keyword_positions:\n",
    "        return {}\n",
    "    \n",
    "    # Get attention from last token to these positions\n",
    "    last_layer_attn = attentions[-1]  # [n_heads, seq_len, seq_len]\n",
    "    last_token_attn = last_layer_attn[:, -1, :]  # [n_heads, seq_len]\n",
    "    mean_attn = last_token_attn.mean(axis=0)  # [seq_len]\n",
    "    \n",
    "    results = {}\n",
    "    for pos, token, kw in keyword_positions:\n",
    "        results[f\"{kw} ('{token}')\"] = mean_attn[pos]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention to numbers and operators\n",
    "KEYWORDS = [\"15\", \"27\", \"plus\", \"step\", \"expert\", \"math\", \"Question\", \"Answer\"]\n",
    "\n",
    "print(\"=== Attention to Key Tokens (from last position) ===\")\n",
    "\n",
    "for name, data in attention_data.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    attn_to_keywords = compute_attention_to_keywords(\n",
    "        data[\"attentions\"], data[\"tokens\"], KEYWORDS\n",
    "    )\n",
    "    \n",
    "    if attn_to_keywords:\n",
    "        for kw, attn in sorted(attn_to_keywords.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {kw:30s}: {attn:.4f}\")\n",
    "    else:\n",
    "        print(\"  No matching keywords found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Layer-wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_entropy(attentions):\n",
    "    \"\"\"\n",
    "    Compute attention entropy per layer.\n",
    "    Higher entropy = more distributed attention.\n",
    "    \"\"\"\n",
    "    n_layers = attentions.shape[0]\n",
    "    entropies = []\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        # Average across heads, look at last token's attention\n",
    "        layer_attn = attentions[layer].mean(axis=0)[-1]  # [seq_len]\n",
    "        \n",
    "        # Compute entropy\n",
    "        layer_attn = layer_attn + 1e-10  # Avoid log(0)\n",
    "        entropy = -np.sum(layer_attn * np.log(layer_attn))\n",
    "        entropies.append(entropy)\n",
    "    \n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare layer-wise attention entropy across prompts\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for name, data in attention_data.items():\n",
    "    entropies = compute_layer_entropy(data[\"attentions\"])\n",
    "    ax.plot(range(len(entropies)), entropies, 'o-', label=name, linewidth=2, markersize=6)\n",
    "\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Attention Entropy')\n",
    "ax.set_title('Attention Distribution Across Layers\\n(Higher = More Distributed)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp8_layer_entropy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Few-Shot Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attention patterns in 0-shot vs few-shot\n",
    "FEWSHOT_PROMPTS = {\n",
    "    \"0-shot\": \"\"\"What is 15 plus 27?\"\"\",\n",
    "    \n",
    "    \"1-shot\": \"\"\"Q: What is 3 plus 4?\n",
    "A: 7\n",
    "\n",
    "Q: What is 15 plus 27?\n",
    "A:\"\"\",\n",
    "    \n",
    "    \"2-shot\": \"\"\"Q: What is 3 plus 4?\n",
    "A: 7\n",
    "\n",
    "Q: What is 10 plus 5?\n",
    "A: 15\n",
    "\n",
    "Q: What is 15 plus 27?\n",
    "A:\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention for few-shot variants\n",
    "fewshot_attention = {}\n",
    "\n",
    "for name, prompt in FEWSHOT_PROMPTS.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    attentions, tokens = get_attention_patterns(model, prompt)\n",
    "    fewshot_attention[name] = {\n",
    "        \"attentions\": attentions,\n",
    "        \"tokens\": tokens,\n",
    "        \"prompt\": prompt\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze: Does the model attend to previous answers in few-shot?\n",
    "print(\"=== Attention to Example Answers in Few-Shot ===\")\n",
    "\n",
    "for name, data in fewshot_attention.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    tokens = data[\"tokens\"]\n",
    "    attentions = data[\"attentions\"]\n",
    "    \n",
    "    # Find positions of answer tokens (7, 15, etc.)\n",
    "    answer_keywords = [\"7\", \"15\"]\n",
    "    attn_to_answers = compute_attention_to_keywords(attentions, tokens, answer_keywords)\n",
    "    \n",
    "    if attn_to_answers:\n",
    "        total_answer_attn = sum(attn_to_answers.values())\n",
    "        print(f\"  Total attention to example answers: {total_answer_attn:.4f}\")\n",
    "        for kw, attn in attn_to_answers.items():\n",
    "            print(f\"    {kw}: {attn:.4f}\")\n",
    "    else:\n",
    "        print(\"  No example answers found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Head Specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_head_specialization(attentions, tokens):\n",
    "    \"\"\"\n",
    "    Analyze what different attention heads focus on.\n",
    "    \"\"\"\n",
    "    n_layers, n_heads = attentions.shape[:2]\n",
    "    \n",
    "    head_stats = []\n",
    "    for layer in range(n_layers):\n",
    "        for head in range(n_heads):\n",
    "            head_attn = attentions[layer, head, -1, :]  # Last token's attention\n",
    "            \n",
    "            # Compute statistics\n",
    "            entropy = -np.sum(head_attn * np.log(head_attn + 1e-10))\n",
    "            max_attn = np.max(head_attn)\n",
    "            max_pos = np.argmax(head_attn)\n",
    "            max_token = tokens[max_pos] if max_pos < len(tokens) else \"[UNK]\"\n",
    "            \n",
    "            head_stats.append({\n",
    "                \"layer\": layer,\n",
    "                \"head\": head,\n",
    "                \"entropy\": entropy,\n",
    "                \"max_attention\": max_attn,\n",
    "                \"focus_position\": max_pos,\n",
    "                \"focus_token\": max_token\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(head_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze head specialization for CoT prompt\n",
    "cot_data = attention_data[\"cot\"]\n",
    "head_df = analyze_head_specialization(cot_data[\"attentions\"], cot_data[\"tokens\"])\n",
    "\n",
    "print(\"=== Head Specialization Analysis (CoT prompt) ===\")\n",
    "\n",
    "# Find heads with lowest entropy (most focused)\n",
    "print(\"\\nMost focused heads (lowest entropy):\")\n",
    "focused_heads = head_df.nsmallest(5, 'entropy')\n",
    "for _, row in focused_heads.iterrows():\n",
    "    print(f\"  Layer {row['layer']}, Head {row['head']}: entropy={row['entropy']:.3f}, focuses on '{row['focus_token']}'\")\n",
    "\n",
    "# Find heads with highest entropy (most distributed)\n",
    "print(\"\\nMost distributed heads (highest entropy):\")\n",
    "distributed_heads = head_df.nlargest(5, 'entropy')\n",
    "for _, row in distributed_heads.iterrows():\n",
    "    print(f\"  Layer {row['layer']}, Head {row['head']}: entropy={row['entropy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 8 SUMMARY: Attention Pattern Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Attention Pattern Differences:\")\n",
    "for name, data in attention_data.items():\n",
    "    entropy = compute_layer_entropy(data[\"attentions\"])[-1]  # Last layer\n",
    "    print(f\"   {name}: last layer entropy = {entropy:.3f}\")\n",
    "\n",
    "print(\"\\n2. Few-Shot Attention Patterns:\")\n",
    "print(\"   [Fill after running: Does model attend to example answers?]\")\n",
    "\n",
    "print(\"\\n3. Key Insights:\")\n",
    "print(\"   - [Fill after running: How do prompts change attention?]\")\n",
    "print(\"   - [Fill after running: Which layers are most affected?]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "save_data = {\n",
    "    \"layer_entropies\": {\n",
    "        name: compute_layer_entropy(data[\"attentions\"])\n",
    "        for name, data in attention_data.items()\n",
    "    },\n",
    "    \"head_specialization_summary\": {\n",
    "        \"most_focused\": focused_heads[['layer', 'head', 'entropy', 'focus_token']].to_dict('records'),\n",
    "        \"most_distributed\": distributed_heads[['layer', 'head', 'entropy']].to_dict('records')\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../results/exp8_attention_results.json', 'w') as f:\n",
    "    json.dump(save_data, f, indent=2, default=float)\n",
    "\n",
    "print(\"Results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
