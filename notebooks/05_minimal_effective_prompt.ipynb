{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5: The Minimal Effective Prompt\n",
    "\n",
    "**Goal:** Find the smallest prompt modification that produces a given performance gain.\n",
    "\n",
    "**Key Questions:**\n",
    "- What is the minimal change needed to improve performance?\n",
    "- Are there universal \"magic words\" that always help?\n",
    "- Can we predict prompt effectiveness from simple features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.metrics import ExperimentResults\n",
    "from src.visualization import set_style\n",
    "\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Candidate Additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_ADDITIONS = {\n",
    "    \"please\": \"Please \",\n",
    "    \"carefully\": \"Carefully \",\n",
    "    \"correctly\": \"Correctly \",\n",
    "    \"step_by_step\": \"Step by step, \",\n",
    "    \"think_carefully\": \"Think carefully. \",\n",
    "    \"lets_think\": \"Let's think. \",\n",
    "    \"be_precise\": \"Be precise. \",\n",
    "    \"expert\": \"As an expert, \",\n",
    "    \"assistant\": \"As a helpful assistant, \",\n",
    "    \"answer_colon\": \"Answer: \",\n",
    "    \"important\": \"Important: \",\n",
    "    \"remember\": \"Remember: \"\n",
    "}\n",
    "\n",
    "TEST_QUESTIONS = [\n",
    "    {\"q\": \"What is 15 + 28?\", \"a\": \"43\"},\n",
    "    {\"q\": \"What is the capital of Japan?\", \"a\": \"Tokyo\"},\n",
    "    {\"q\": \"Is 'happy' a positive or negative word?\", \"a\": \"positive\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single Addition Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_additions(model, question, expected, additions):\n",
    "    \"\"\"Test impact of adding each candidate to the base prompt.\"\"\"\n",
    "    base_prompt = question\n",
    "    baseline_probs = model.get_sequence_log_probs(base_prompt, \" \" + expected)\n",
    "    baseline = baseline_probs[\"total_log_prob\"]\n",
    "    \n",
    "    results = [{\"addition\": \"baseline\", \"prompt\": base_prompt, \"log_prob\": baseline, \"improvement\": 0}]\n",
    "    \n",
    "    for name, addition in additions.items():\n",
    "        prefix_prompt = addition + base_prompt\n",
    "        prefix_probs = model.get_sequence_log_probs(prefix_prompt, \" \" + expected)\n",
    "        \n",
    "        results.append({\n",
    "            \"addition\": name,\n",
    "            \"prompt\": prefix_prompt,\n",
    "            \"log_prob\": prefix_probs[\"total_log_prob\"],\n",
    "            \"improvement\": prefix_probs[\"total_log_prob\"] - baseline\n",
    "        })\n",
    "    \n",
    "    return results, baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_addition_results = []\n",
    "\n",
    "for test in tqdm(TEST_QUESTIONS, desc=\"Testing questions\"):\n",
    "    results, baseline = test_single_additions(model, test[\"q\"], test[\"a\"], CANDIDATE_ADDITIONS)\n",
    "    for r in results:\n",
    "        r[\"question\"] = test[\"q\"]\n",
    "        r[\"baseline\"] = baseline\n",
    "    all_addition_results.extend(results)\n",
    "\n",
    "df = pd.DataFrame(all_addition_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Average Improvement by Addition ===\")\n",
    "improvements = df[df['addition'] != 'baseline'].groupby('addition')['improvement'].agg(['mean', 'std'])\n",
    "improvements = improvements.sort_values('mean', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5:\")\n",
    "for idx, row in improvements.head(5).iterrows():\n",
    "    print(f\"  {idx:20s}: mean={row['mean']:+.4f}\")\n",
    "\n",
    "print(\"\\nWorst 3:\")\n",
    "for idx, row in improvements.tail(3).iterrows():\n",
    "    print(f\"  {idx:20s}: mean={row['mean']:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "additions = improvements.index.tolist()\n",
    "means = improvements['mean'].values\n",
    "colors = ['green' if m > 0 else 'red' for m in means]\n",
    "\n",
    "ax.barh(range(len(additions)), means, color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(additions)))\n",
    "ax.set_yticklabels(additions)\n",
    "ax.set_xlabel('Improvement in Log Probability')\n",
    "ax.set_title('Impact of Single-Word/Phrase Additions')\n",
    "ax.axvline(x=0, color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp5_single_additions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combination Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_addition_combinations(model, question, expected, additions, max_combo_size=2):\n",
    "    baseline = model.get_sequence_log_probs(question, \" \" + expected)[\"total_log_prob\"]\n",
    "    results = []\n",
    "    addition_items = list(additions.items())\n",
    "    \n",
    "    for combo in combinations(addition_items, max_combo_size):\n",
    "        names = [c[0] for c in combo]\n",
    "        texts = [c[1] for c in combo]\n",
    "        prompt = \"\".join(texts) + question\n",
    "        probs = model.get_sequence_log_probs(prompt, \" \" + expected)\n",
    "        results.append({\n",
    "            \"combination\": \"+\".join(names),\n",
    "            \"log_prob\": probs[\"total_log_prob\"],\n",
    "            \"improvement\": probs[\"total_log_prob\"] - baseline\n",
    "        })\n",
    "    return results, baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TEST_QUESTIONS[0]\n",
    "top_additions = dict(list(CANDIDATE_ADDITIONS.items())[:6])\n",
    "\n",
    "print(f\"Testing combinations for: {test['q']}\")\n",
    "combo_results, baseline = test_addition_combinations(model, test[\"q\"], test[\"a\"], top_additions)\n",
    "combo_results = sorted(combo_results, key=lambda x: x[\"improvement\"], reverse=True)\n",
    "\n",
    "print(f\"\\nTop 10 combinations:\")\n",
    "for r in combo_results[:10]:\n",
    "    print(f\"  {r['combination']:35s}: {r['improvement']:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Minimal Substring Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimal_effective_substring(model, question, expected, effective_addition):\n",
    "    baseline = model.get_sequence_log_probs(question, \" \" + expected)[\"total_log_prob\"]\n",
    "    full_prompt = effective_addition + question\n",
    "    full_improvement = model.get_sequence_log_probs(full_prompt, \" \" + expected)[\"total_log_prob\"] - baseline\n",
    "    \n",
    "    results = []\n",
    "    words = effective_addition.split()\n",
    "    \n",
    "    for length in range(len(words), 0, -1):\n",
    "        for start in range(len(words) - length + 1):\n",
    "            substring = \" \".join(words[start:start+length]) + \" \"\n",
    "            prompt = substring + question\n",
    "            improvement = model.get_sequence_log_probs(prompt, \" \" + expected)[\"total_log_prob\"] - baseline\n",
    "            results.append({\n",
    "                \"substring\": substring.strip(),\n",
    "                \"length\": length,\n",
    "                \"improvement\": improvement,\n",
    "                \"pct_of_full\": (improvement / full_improvement * 100) if full_improvement != 0 else 0\n",
    "            })\n",
    "    return results, full_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TEST_QUESTIONS[0]\n",
    "effective_phrase = \"Let's think step by step. \"\n",
    "\n",
    "substring_results, full_improvement = find_minimal_effective_substring(\n",
    "    model, test[\"q\"], test[\"a\"], effective_phrase\n",
    ")\n",
    "\n",
    "print(f\"Full phrase improvement: {full_improvement:.4f}\\n\")\n",
    "sorted_results = sorted(substring_results, key=lambda x: x[\"improvement\"], reverse=True)\n",
    "\n",
    "for r in sorted_results[:10]:\n",
    "    print(f\"  '{r['substring']:20s}' (len={r['length']}): {r['improvement']:+.4f} ({r['pct_of_full']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt_features(prompt):\n",
    "    return {\n",
    "        \"length_chars\": len(prompt),\n",
    "        \"length_words\": len(prompt.split()),\n",
    "        \"has_colon\": \":\" in prompt,\n",
    "        \"has_please\": \"please\" in prompt.lower(),\n",
    "        \"has_step\": \"step\" in prompt.lower(),\n",
    "        \"has_think\": \"think\" in prompt.lower(),\n",
    "        \"has_expert\": \"expert\" in prompt.lower(),\n",
    "    }\n",
    "\n",
    "features_list = []\n",
    "for _, row in df.iterrows():\n",
    "    if 'prompt' in row:\n",
    "        features = extract_prompt_features(row['prompt'])\n",
    "        features['log_prob'] = row.get('log_prob', 0)\n",
    "        features_list.append(features)\n",
    "\n",
    "feature_df = pd.DataFrame(features_list)\n",
    "correlations = {col: feature_df[col].astype(float).corr(feature_df['log_prob'])\n",
    "                for col in feature_df.columns if col != 'log_prob'}\n",
    "\n",
    "print(\"=== Feature Correlations ===\")\n",
    "for f, c in sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True):\n",
    "    if not np.isnan(c):\n",
    "        print(f\"  {f:20s}: {c:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 5 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Best Single Additions:\")\n",
    "for idx, row in improvements.head(3).iterrows():\n",
    "    print(f\"   '{idx}': {row['mean']:+.4f}\")\n",
    "\n",
    "print(\"\\n2. Key Findings:\")\n",
    "print(\"   - [Fill after running: Which additions are most effective?]\")\n",
    "print(\"   - [Fill after running: Are effects consistent across questions?]\")\n",
    "print(\"   - [Fill after running: What features predict success?]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "save_data = {\n",
    "    \"single_additions\": improvements.to_dict(),\n",
    "    \"feature_correlations\": {k: v for k, v in correlations.items() if not np.isnan(v)}\n",
    "}\n",
    "\n",
    "with open('../results/exp5_minimal_prompt_results.json', 'w') as f:\n",
    "    json.dump(save_data, f, indent=2, default=float)\n",
    "\n",
    "print(\"Results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
