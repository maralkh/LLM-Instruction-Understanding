{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 10: Predictive Framework for Prompt Effectiveness\n",
    "\n",
    "**Goal:** Build a model to predict prompt effectiveness without full evaluation.\n",
    "\n",
    "**Key Questions:**\n",
    "- Can we predict effectiveness from prompt features alone?\n",
    "- Which features are most predictive?\n",
    "- Can we build a simple scoring heuristic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.visualization import set_style\n",
    "\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Training Data\n",
    "\n",
    "We need diverse prompts with measured effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components to mix\n",
    "PERSONAS = [\"\", \"You are an expert.\", \"You are a helpful assistant.\", \"You are a teacher.\", \"As a specialist,\"]\n",
    "INSTRUCTIONS = [\"\", \"Answer accurately.\", \"Be precise.\", \"Think carefully.\", \"Respond concisely.\"]\n",
    "COT_TRIGGERS = [\"\", \"Let's think step by step.\", \"Let's work through this.\", \"Step by step:\"]\n",
    "FORMATS = [\n",
    "    \"{q}\",\n",
    "    \"Question: {q}\\nAnswer:\",\n",
    "    \"Q: {q}\\nA:\",\n",
    "    \"### Question\\n{q}\\n### Answer\"\n",
    "]\n",
    "\n",
    "QUESTIONS = [\n",
    "    {\"q\": \"What is 25 + 37?\", \"a\": \"62\"},\n",
    "    {\"q\": \"What is the capital of Italy?\", \"a\": \"Rome\"},\n",
    "    {\"q\": \"Is 'wonderful' positive or negative?\", \"a\": \"positive\"},\n",
    "    {\"q\": \"What is 8 * 9?\", \"a\": \"72\"},\n",
    "    {\"q\": \"What color is the sky?\", \"a\": \"blue\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(persona, instruction, cot, fmt, question):\n",
    "    \"\"\"Generate a prompt from components.\"\"\"\n",
    "    parts = [p for p in [persona, instruction] if p]\n",
    "    prefix = \" \".join(parts)\n",
    "    \n",
    "    formatted_q = fmt.format(q=question)\n",
    "    \n",
    "    if prefix:\n",
    "        prompt = f\"{prefix}\\n\\n{formatted_q}\"\n",
    "    else:\n",
    "        prompt = formatted_q\n",
    "    \n",
    "    if cot:\n",
    "        prompt = f\"{prompt}\\n\\n{cot}\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate diverse prompts and measure effectiveness\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "training_data = []\n",
    "n_samples = 200  # Sample from the combinatorial space\n",
    "\n",
    "for _ in tqdm(range(n_samples), desc=\"Generating samples\"):\n",
    "    # Random components\n",
    "    persona = random.choice(PERSONAS)\n",
    "    instruction = random.choice(INSTRUCTIONS)\n",
    "    cot = random.choice(COT_TRIGGERS)\n",
    "    fmt = random.choice(FORMATS)\n",
    "    qa = random.choice(QUESTIONS)\n",
    "    \n",
    "    prompt = generate_prompt(persona, instruction, cot, fmt, qa[\"q\"])\n",
    "    \n",
    "    # Measure effectiveness\n",
    "    seq_probs = model.get_sequence_log_probs(prompt, \" \" + qa[\"a\"])\n",
    "    dist = model.get_next_token_distribution(prompt)\n",
    "    \n",
    "    training_data.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"persona\": persona,\n",
    "        \"instruction\": instruction,\n",
    "        \"cot\": cot,\n",
    "        \"format\": fmt,\n",
    "        \"question\": qa[\"q\"],\n",
    "        \"target_log_prob\": seq_probs[\"total_log_prob\"],\n",
    "        \"entropy\": dist[\"entropy\"]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(training_data)\n",
    "print(f\"Generated {len(df)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(row):\n",
    "    \"\"\"Extract predictive features from a prompt.\"\"\"\n",
    "    prompt = row[\"prompt\"]\n",
    "    \n",
    "    features = {\n",
    "        # Length features\n",
    "        \"prompt_length\": len(prompt),\n",
    "        \"word_count\": len(prompt.split()),\n",
    "        \"line_count\": prompt.count(\"\\n\") + 1,\n",
    "        \n",
    "        # Component presence\n",
    "        \"has_persona\": 1 if row[\"persona\"] else 0,\n",
    "        \"has_instruction\": 1 if row[\"instruction\"] else 0,\n",
    "        \"has_cot\": 1 if row[\"cot\"] else 0,\n",
    "        \n",
    "        # Keyword presence\n",
    "        \"has_expert\": 1 if \"expert\" in prompt.lower() else 0,\n",
    "        \"has_step\": 1 if \"step\" in prompt.lower() else 0,\n",
    "        \"has_think\": 1 if \"think\" in prompt.lower() else 0,\n",
    "        \"has_careful\": 1 if \"careful\" in prompt.lower() else 0,\n",
    "        \"has_accurate\": 1 if \"accura\" in prompt.lower() else 0,\n",
    "        \"has_precise\": 1 if \"precise\" in prompt.lower() else 0,\n",
    "        \n",
    "        # Format features\n",
    "        \"has_qa_format\": 1 if \"Question:\" in prompt or \"Q:\" in prompt else 0,\n",
    "        \"has_markdown\": 1 if \"###\" in prompt else 0,\n",
    "        \"has_colon\": prompt.count(\":\"),\n",
    "        \"has_newlines\": prompt.count(\"\\n\"),\n",
    "        \n",
    "        # Punctuation\n",
    "        \"ends_with_colon\": 1 if prompt.strip().endswith(\":\") else 0,\n",
    "        \"ends_with_period\": 1 if prompt.strip().endswith(\".\") else 0,\n",
    "        \"question_marks\": prompt.count(\"?\"),\n",
    "        \n",
    "        # Complexity\n",
    "        \"avg_word_length\": np.mean([len(w) for w in prompt.split()]) if prompt.split() else 0,\n",
    "        \"unique_words\": len(set(prompt.lower().split())),\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all samples\n",
    "feature_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    features = extract_features(row)\n",
    "    features[\"target\"] = row[\"target_log_prob\"]\n",
    "    feature_rows.append(features)\n",
    "\n",
    "feature_df = pd.DataFrame(feature_rows)\n",
    "print(f\"Extracted {len(feature_df.columns) - 1} features\")\n",
    "print(feature_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Predictive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "feature_cols = [c for c in feature_df.columns if c != \"target\"]\n",
    "X = feature_df[feature_cols].values\n",
    "y = feature_df[\"target\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {len(X_train)}, Test set: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models_to_test = {\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model_obj in models_to_test.items():\n",
    "    # Fit\n",
    "    if name == \"RandomForest\":\n",
    "        model_obj.fit(X_train, y_train)\n",
    "        y_pred = model_obj.predict(X_test)\n",
    "    else:\n",
    "        model_obj.fit(X_train_scaled, y_train)\n",
    "        y_pred = model_obj.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\"model\": model_obj, \"r2\": r2, \"mae\": mae, \"y_pred\": y_pred}\n",
    "    print(f\"{name}: R² = {r2:.4f}, MAE = {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(y_test, res[\"y_pred\"], alpha=0.5)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('Actual Log Probability')\n",
    "    ax.set_ylabel('Predicted Log Probability')\n",
    "    ax.set_title(f'{name}\\nR² = {res[\"r2\"]:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp10_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from different models\n",
    "importances = {}\n",
    "\n",
    "# Ridge coefficients\n",
    "ridge_importance = pd.Series(\n",
    "    np.abs(results[\"Ridge\"][\"model\"].coef_),\n",
    "    index=feature_cols\n",
    ").sort_values(ascending=False)\n",
    "importances[\"Ridge\"] = ridge_importance\n",
    "\n",
    "# Lasso coefficients (sparse)\n",
    "lasso_importance = pd.Series(\n",
    "    np.abs(results[\"Lasso\"][\"model\"].coef_),\n",
    "    index=feature_cols\n",
    ").sort_values(ascending=False)\n",
    "importances[\"Lasso\"] = lasso_importance\n",
    "\n",
    "# Random Forest\n",
    "rf_importance = pd.Series(\n",
    "    results[\"RandomForest\"][\"model\"].feature_importances_,\n",
    "    index=feature_cols\n",
    ").sort_values(ascending=False)\n",
    "importances[\"RandomForest\"] = rf_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importances\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "for idx, (name, imp) in enumerate(importances.items()):\n",
    "    ax = axes[idx]\n",
    "    top_10 = imp.head(10)\n",
    "    ax.barh(range(len(top_10)), top_10.values, color='steelblue', alpha=0.7)\n",
    "    ax.set_yticks(range(len(top_10)))\n",
    "    ax.set_yticklabels(top_10.index)\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'{name} - Top 10 Features')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp10_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consensus important features\n",
    "print(\"=== Feature Importance Consensus ===\")\n",
    "\n",
    "# Rank features in each model and average ranks\n",
    "ranks = pd.DataFrame({\n",
    "    name: imp.rank(ascending=False)\n",
    "    for name, imp in importances.items()\n",
    "})\n",
    "avg_rank = ranks.mean(axis=1).sort_values()\n",
    "\n",
    "print(\"\\nTop 10 features (by average rank across models):\")\n",
    "for feat, rank in avg_rank.head(10).items():\n",
    "    print(f\"  {feat:25s}: avg rank = {rank:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Simple Scoring Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Lasso to identify key features (automatic feature selection)\n",
    "lasso_nonzero = lasso_importance[lasso_importance > 0.01]\n",
    "print(\"=== Features Selected by Lasso ===\")\n",
    "print(lasso_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple scoring function based on top features\n",
    "def simple_prompt_score(prompt):\n",
    "    \"\"\"\n",
    "    Simple heuristic score for prompt effectiveness.\n",
    "    Based on learned feature importance.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Positive factors (adjust weights based on your findings)\n",
    "    if \"step\" in prompt.lower():\n",
    "        score += 2\n",
    "    if \"expert\" in prompt.lower():\n",
    "        score += 1\n",
    "    if \"Question:\" in prompt or \"Q:\" in prompt:\n",
    "        score += 1\n",
    "    if prompt.strip().endswith(\":\"):\n",
    "        score += 1\n",
    "    \n",
    "    # Moderate prompt length is good\n",
    "    word_count = len(prompt.split())\n",
    "    if 10 <= word_count <= 50:\n",
    "        score += 1\n",
    "    elif word_count > 100:\n",
    "        score -= 1\n",
    "    \n",
    "    # Structure helps\n",
    "    if prompt.count(\"\\n\") >= 2:\n",
    "        score += 1\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate heuristic\n",
    "df[\"heuristic_score\"] = df[\"prompt\"].apply(simple_prompt_score)\n",
    "\n",
    "corr = df[\"heuristic_score\"].corr(df[\"target_log_prob\"])\n",
    "print(f\"Correlation between heuristic score and actual performance: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare heuristic groups\n",
    "df[\"score_group\"] = pd.cut(df[\"heuristic_score\"], bins=[-np.inf, 1, 3, np.inf], labels=[\"Low\", \"Medium\", \"High\"])\n",
    "\n",
    "print(\"\\n=== Performance by Heuristic Score Group ===\")\n",
    "for group in [\"Low\", \"Medium\", \"High\"]:\n",
    "    subset = df[df[\"score_group\"] == group]\n",
    "    print(f\"  {group}: n={len(subset)}, mean log_prob={subset['target_log_prob'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 10 SUMMARY: Predictive Framework\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Model Performance:\")\n",
    "best_model = max(results.items(), key=lambda x: x[1][\"r2\"])\n",
    "print(f\"   Best model: {best_model[0]} (R² = {best_model[1]['r2']:.3f})\")\n",
    "\n",
    "print(\"\\n2. Most Predictive Features:\")\n",
    "for feat in avg_rank.head(5).index:\n",
    "    print(f\"   - {feat}\")\n",
    "\n",
    "print(\"\\n3. Simple Heuristic:\")\n",
    "print(f\"   Correlation with actual: {corr:.3f}\")\n",
    "print(\"   Key factors: step-by-step, Q/A format, structured layout\")\n",
    "\n",
    "print(\"\\n4. Practical Guidelines:\")\n",
    "print(\"   - [Fill after running: What predicts good prompts?]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "save_data = {\n",
    "    \"model_performance\": {name: {\"r2\": res[\"r2\"], \"mae\": res[\"mae\"]} for name, res in results.items()},\n",
    "    \"top_features\": avg_rank.head(10).to_dict(),\n",
    "    \"heuristic_correlation\": float(corr)\n",
    "}\n",
    "\n",
    "with open('../results/exp10_predictive_framework.json', 'w') as f:\n",
    "    json.dump(save_data, f, indent=2)\n",
    "\n",
    "print(\"Results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
