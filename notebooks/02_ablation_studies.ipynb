{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: System Prompt Ablation Studies\n",
    "\n",
    "**Goal:** Identify which components of system prompts carry the most causal weight.\n",
    "\n",
    "**Setup:**\n",
    "- Fixed test prompts\n",
    "- Take complex system prompts and systematically remove components\n",
    "- Measure impact of each component removal\n",
    "\n",
    "**Key Questions:**\n",
    "- Which parts of system prompts matter most?\n",
    "- Are some components redundant?\n",
    "- What's the minimal effective system prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.metrics import DistributionMetrics\n",
    "from src.visualization import set_style\n",
    "from src.test_configs import ALL_TEST_PROMPTS, build_chat_prompt\n",
    "\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Component-Based System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A complex system prompt broken into components\n",
    "COMPONENTS = {\n",
    "    \"role\": \"You are a helpful AI assistant.\",\n",
    "    \"expertise\": \"You have expertise across many domains.\",\n",
    "    \"accuracy\": \"Always provide accurate information.\",\n",
    "    \"honesty\": \"Be honest about uncertainty.\",\n",
    "    \"reasoning\": \"Think step by step before answering.\",\n",
    "    \"format\": \"Be clear and concise in your responses.\",\n",
    "    \"safety\": \"Avoid harmful or misleading content.\",\n",
    "}\n",
    "\n",
    "def build_system_prompt(include_components):\n",
    "    \"\"\"Build system prompt from selected components.\"\"\"\n",
    "    parts = [COMPONENTS[c] for c in include_components if c in COMPONENTS]\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# Full system prompt with all components\n",
    "FULL_SYSTEM = build_system_prompt(COMPONENTS.keys())\n",
    "print(f\"Full system prompt ({len(COMPONENTS)} components):\\n{FULL_SYSTEM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single Component Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_experiment(model, test_prompts, components, n_test=10):\n",
    "    \"\"\"Run ablation: measure effect of removing each component.\"\"\"\n",
    "    test_subset = test_prompts[:n_test]\n",
    "    \n",
    "    # Get baseline (full system prompt)\n",
    "    full_prompt = build_system_prompt(components.keys())\n",
    "    baseline_results = []\n",
    "    \n",
    "    for test in test_subset:\n",
    "        prompt = build_chat_prompt(full_prompt, test[\"prompt\"], model.tokenizer)\n",
    "        dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "        baseline_results.append({\n",
    "            \"test_id\": test[\"id\"],\n",
    "            \"full_probs\": dist[\"full_probs\"],\n",
    "            \"entropy\": dist[\"entropy\"],\n",
    "            \"top_token\": dist[\"top_tokens\"][0]\n",
    "        })\n",
    "    \n",
    "    # Ablate each component\n",
    "    ablation_results = []\n",
    "    \n",
    "    for remove_component in tqdm(components.keys(), desc=\"Ablating components\"):\n",
    "        # Build system prompt without this component\n",
    "        remaining = [c for c in components.keys() if c != remove_component]\n",
    "        ablated_prompt = build_system_prompt(remaining)\n",
    "        \n",
    "        for i, test in enumerate(test_subset):\n",
    "            prompt = build_chat_prompt(ablated_prompt, test[\"prompt\"], model.tokenizer)\n",
    "            dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "            \n",
    "            # Compare to baseline\n",
    "            js = DistributionMetrics.jensen_shannon(\n",
    "                baseline_results[i][\"full_probs\"], dist[\"full_probs\"]\n",
    "            )\n",
    "            \n",
    "            ablation_results.append({\n",
    "                \"removed_component\": remove_component,\n",
    "                \"test_id\": test[\"id\"],\n",
    "                \"js_divergence\": js,\n",
    "                \"entropy_change\": dist[\"entropy\"] - baseline_results[i][\"entropy\"],\n",
    "                \"top_token_changed\": dist[\"top_tokens\"][0] != baseline_results[i][\"top_token\"]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(ablation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_df = run_ablation_experiment(model, ALL_TEST_PROMPTS, COMPONENTS, n_test=15)\n",
    "\n",
    "# Aggregate by component\n",
    "component_impact = ablation_df.groupby('removed_component').agg({\n",
    "    'js_divergence': ['mean', 'std'],\n",
    "    'entropy_change': 'mean',\n",
    "    'top_token_changed': 'mean'\n",
    "}).round(4)\n",
    "component_impact.columns = ['js_mean', 'js_std', 'entropy_change', 'top_change_rate']\n",
    "component_impact = component_impact.sort_values('js_mean', ascending=False)\n",
    "\n",
    "print(\"=== Component Impact (removal effect) ===\")\n",
    "print(\"Higher JS = component has MORE impact\")\n",
    "print(component_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "c = component_impact.sort_values('js_mean')\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(c)))\n",
    "ax.barh(range(len(c)), c['js_mean'], xerr=c['js_std'], color=colors, capsize=3)\n",
    "ax.set_yticks(range(len(c)))\n",
    "ax.set_yticklabels([f\"{idx}\\n\\\"{COMPONENTS[idx][:30]}...\\\"\" for idx in c.index])\n",
    "ax.set_xlabel('JS Divergence When Removed')\n",
    "ax.set_title('Component Importance (Higher = More Important)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp2_component_importance.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cumulative Ablation (Remove Multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_ablation(model, test_prompts, components, order, n_test=10):\n",
    "    \"\"\"Remove components one by one in specified order.\"\"\"\n",
    "    test_subset = test_prompts[:n_test]\n",
    "    all_components = list(components.keys())\n",
    "    \n",
    "    # Baseline: no system prompt\n",
    "    no_system_results = []\n",
    "    for test in test_subset:\n",
    "        prompt = build_chat_prompt(\"\", test[\"prompt\"], model.tokenizer)\n",
    "        dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "        no_system_results.append(dist[\"entropy\"])\n",
    "    \n",
    "    results = [{\"n_components\": 0, \"components\": \"none\", \"mean_entropy\": np.mean(no_system_results)}]\n",
    "    \n",
    "    # Add components one by one\n",
    "    current_components = []\n",
    "    for comp in order:\n",
    "        current_components.append(comp)\n",
    "        sys_prompt = build_system_prompt(current_components)\n",
    "        \n",
    "        entropies = []\n",
    "        for test in test_subset:\n",
    "            prompt = build_chat_prompt(sys_prompt, test[\"prompt\"], model.tokenizer)\n",
    "            dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "            entropies.append(dist[\"entropy\"])\n",
    "        \n",
    "        results.append({\n",
    "            \"n_components\": len(current_components),\n",
    "            \"components\": \"+\".join(current_components),\n",
    "            \"last_added\": comp,\n",
    "            \"mean_entropy\": np.mean(entropies)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different orderings\n",
    "# Order by importance (most important first)\n",
    "importance_order = component_impact.index.tolist()\n",
    "\n",
    "cumulative_df = cumulative_ablation(model, ALL_TEST_PROMPTS, COMPONENTS, importance_order, n_test=10)\n",
    "print(cumulative_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(cumulative_df['n_components'], cumulative_df['mean_entropy'], 'o-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Mean Entropy')\n",
    "ax.set_title('Effect of Adding System Prompt Components')\n",
    "\n",
    "# Annotate\n",
    "for i, row in cumulative_df.iterrows():\n",
    "    if i > 0:\n",
    "        ax.annotate(row.get('last_added', ''), (row['n_components'], row['mean_entropy']),\n",
    "                   textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp2_cumulative.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Surface-Level Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if surface changes to system prompt matter\n",
    "SURFACE_VARIANTS = {\n",
    "    \"original\": FULL_SYSTEM,\n",
    "    \"lowercase\": FULL_SYSTEM.lower(),\n",
    "    \"uppercase\": FULL_SYSTEM.upper(),\n",
    "    \"no_periods\": FULL_SYSTEM.replace(\".\", \"\"),\n",
    "    \"extra_spaces\": FULL_SYSTEM.replace(\" \", \"  \"),\n",
    "    \"newlines\": FULL_SYSTEM.replace(\". \", \".\\n\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_results = []\n",
    "test_subset = ALL_TEST_PROMPTS[:10]\n",
    "\n",
    "# Get original baseline\n",
    "original_dists = []\n",
    "for test in test_subset:\n",
    "    prompt = build_chat_prompt(SURFACE_VARIANTS[\"original\"], test[\"prompt\"], model.tokenizer)\n",
    "    dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "    original_dists.append(dist)\n",
    "\n",
    "# Compare variants\n",
    "for variant_name, variant_text in tqdm(SURFACE_VARIANTS.items()):\n",
    "    if variant_name == \"original\":\n",
    "        continue\n",
    "    \n",
    "    for i, test in enumerate(test_subset):\n",
    "        prompt = build_chat_prompt(variant_text, test[\"prompt\"], model.tokenizer)\n",
    "        dist = model.get_next_token_distribution(prompt, top_k=50)\n",
    "        \n",
    "        js = DistributionMetrics.jensen_shannon(original_dists[i][\"full_probs\"], dist[\"full_probs\"])\n",
    "        \n",
    "        surface_results.append({\n",
    "            \"variant\": variant_name,\n",
    "            \"test_id\": test[\"id\"],\n",
    "            \"js_divergence\": js\n",
    "        })\n",
    "\n",
    "surface_df = pd.DataFrame(surface_results)\n",
    "surface_impact = surface_df.groupby('variant')['js_divergence'].agg(['mean', 'std']).round(4)\n",
    "print(\"=== Surface Modification Impact ===\")\n",
    "print(surface_impact.sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 2 SUMMARY: System Prompt Ablation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. MOST IMPORTANT Components (removing hurts most):\")\n",
    "for comp in component_impact.head(3).index:\n",
    "    print(f\"   • {comp}: {COMPONENTS[comp][:50]}...\")\n",
    "\n",
    "print(\"\\n2. LEAST IMPORTANT Components:\")\n",
    "for comp in component_impact.tail(3).index:\n",
    "    print(f\"   • {comp}: {COMPONENTS[comp][:50]}...\")\n",
    "\n",
    "print(\"\\n3. Surface Modifications Matter?\")\n",
    "if surface_impact['mean'].max() > 0.1:\n",
    "    print(\"   → Yes, formatting affects output\")\n",
    "else:\n",
    "    print(\"   → No, model is robust to surface changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../results/exp2_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"component_impact\": component_impact.to_dict(),\n",
    "        \"surface_impact\": surface_impact.to_dict()\n",
    "    }, f, indent=2, default=float)\n",
    "print(\"Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
