{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5: Persona Consistency Analysis\n",
    "\n",
    "**Goal:** Test how different persona system prompts affect responses to the SAME queries.\n",
    "\n",
    "**Setup:**\n",
    "- Fixed test prompts\n",
    "- Different persona system prompts (teacher, scientist, casual friend, etc.)\n",
    "- Measure: vocabulary, formality, explanation depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.visualization import set_style\n",
    "from src.test_configs import ALL_TEST_PROMPTS, build_chat_prompt\n",
    "\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONAS = {\n",
    "    \"none\": \"\",\n",
    "    \n",
    "    \"teacher\": \"\"\"You are a patient, encouraging teacher. Explain concepts clearly \n",
    "using simple language. Use examples and check understanding.\"\"\",\n",
    "    \n",
    "    \"scientist\": \"\"\"You are a rigorous scientist. Be precise and cite evidence.\n",
    "Distinguish between facts and hypotheses. Quantify uncertainty.\"\"\",\n",
    "    \n",
    "    \"friend\": \"\"\"You're a casual, friendly buddy. Keep it relaxed and fun.\n",
    "Use informal language, slang is okay. Be supportive.\"\"\",\n",
    "    \n",
    "    \"executive\": \"\"\"You are a busy executive. Be extremely concise and direct.\n",
    "Focus on key points only. No fluff, just actionable information.\"\"\",\n",
    "    \n",
    "    \"philosopher\": \"\"\"You are a thoughtful philosopher. Consider multiple perspectives.\n",
    "Ask probing questions. Explore nuance and deeper meaning.\"\"\",\n",
    "    \n",
    "    \"child\": \"\"\"Explain things as if talking to a curious 8-year-old.\n",
    "Use very simple words and fun examples.\"\"\",\n",
    "    \n",
    "    \"expert\": \"\"\"You are a domain expert with deep technical knowledge.\n",
    "Use precise terminology. Assume the user has advanced understanding.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SUBSET = ALL_TEST_PROMPTS[:10]\n",
    "\n",
    "results = []\n",
    "for persona_name, persona_text in tqdm(PERSONAS.items()):\n",
    "    for test in TEST_SUBSET:\n",
    "        prompt = build_chat_prompt(persona_text, test[\"prompt\"], model.tokenizer)\n",
    "        output = model.generate_with_probs(prompt, max_new_tokens=100, temperature=0.7)\n",
    "        \n",
    "        results.append({\n",
    "            \"persona\": persona_name,\n",
    "            \"test_id\": test[\"id\"],\n",
    "            \"category\": test[\"category\"],\n",
    "            \"response\": output[\"generated_text\"],\n",
    "            \"n_tokens\": output[\"n_tokens\"]\n",
    "        })\n",
    "\n",
    "persona_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Style Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formality indicators\n",
    "INFORMAL_WORDS = [\"gonna\", \"wanna\", \"kinda\", \"yeah\", \"yep\", \"nope\", \"cool\", \"awesome\", \"hey\", \"ok\", \"okay\"]\n",
    "FORMAL_WORDS = [\"therefore\", \"however\", \"furthermore\", \"consequently\", \"thus\", \"hence\", \"moreover\"]\n",
    "TECHNICAL_WORDS = [\"hypothesis\", \"analysis\", \"data\", \"evidence\", \"methodology\", \"framework\", \"paradigm\"]\n",
    "SIMPLE_WORDS = [\"like\", \"thing\", \"stuff\", \"lot\", \"very\", \"really\", \"big\", \"small\", \"good\", \"bad\"]\n",
    "\n",
    "def analyze_style(response):\n",
    "    text = response.lower()\n",
    "    words = text.split()\n",
    "    \n",
    "    return {\n",
    "        \"length\": len(words),\n",
    "        \"avg_word_length\": np.mean([len(w) for w in words]) if words else 0,\n",
    "        \"informal_count\": sum(1 for w in INFORMAL_WORDS if w in text),\n",
    "        \"formal_count\": sum(1 for w in FORMAL_WORDS if w in text),\n",
    "        \"technical_count\": sum(1 for w in TECHNICAL_WORDS if w in text),\n",
    "        \"simple_count\": sum(1 for w in SIMPLE_WORDS if w in text),\n",
    "        \"question_count\": response.count(\"?\"),\n",
    "        \"exclamation_count\": response.count(\"!\"),\n",
    "        \"sentence_count\": len(re.split(r'[.!?]+', response)),\n",
    "    }\n",
    "\n",
    "# Apply to all responses\n",
    "style_features = []\n",
    "for _, row in persona_df.iterrows():\n",
    "    feat = analyze_style(row[\"response\"])\n",
    "    feat[\"persona\"] = row[\"persona\"]\n",
    "    style_features.append(feat)\n",
    "\n",
    "style_df = pd.DataFrame(style_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_summary = style_df.groupby('persona').mean().round(2)\n",
    "print(\"=== Persona Style Summary ===\")\n",
    "print(persona_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Response length\n",
    "ax = axes[0, 0]\n",
    "data = persona_summary['length'].sort_values()\n",
    "ax.barh(range(len(data)), data.values, alpha=0.7)\n",
    "ax.set_yticks(range(len(data))); ax.set_yticklabels(data.index)\n",
    "ax.set_xlabel('Avg Words'); ax.set_title('Response Length by Persona')\n",
    "\n",
    "# Formality score (formal - informal)\n",
    "ax = axes[0, 1]\n",
    "formality = (persona_summary['formal_count'] - persona_summary['informal_count']).sort_values()\n",
    "colors = ['green' if x > 0 else 'orange' for x in formality]\n",
    "ax.barh(range(len(formality)), formality.values, color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(formality))); ax.set_yticklabels(formality.index)\n",
    "ax.set_xlabel('Formality Score'); ax.set_title('Formality (Formal - Informal words)')\n",
    "\n",
    "# Avg word length (complexity proxy)\n",
    "ax = axes[1, 0]\n",
    "data = persona_summary['avg_word_length'].sort_values()\n",
    "ax.barh(range(len(data)), data.values, alpha=0.7)\n",
    "ax.set_yticks(range(len(data))); ax.set_yticklabels(data.index)\n",
    "ax.set_xlabel('Avg Word Length'); ax.set_title('Vocabulary Complexity')\n",
    "\n",
    "# Questions asked\n",
    "ax = axes[1, 1]\n",
    "data = persona_summary['question_count'].sort_values()\n",
    "ax.barh(range(len(data)), data.values, alpha=0.7)\n",
    "ax.set_yticks(range(len(data))); ax.set_yticklabels(data.index)\n",
    "ax.set_xlabel('Avg Questions'); ax.set_title('Questions in Response')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/exp5_persona_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show same prompt under different personas\n",
    "sample_test = TEST_SUBSET[0]\n",
    "print(f\"Prompt: {sample_test['prompt']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for persona in ['none', 'teacher', 'scientist', 'friend', 'executive']:\n",
    "    response = persona_df[(persona_df['test_id'] == sample_test['id']) & \n",
    "                          (persona_df['persona'] == persona)]['response'].values[0]\n",
    "    print(f\"\\n[{persona.upper()}]\")\n",
    "    print(response[:200] + \"...\" if len(response) > 200 else response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Key Findings ===\")\n",
    "print(f\"Longest responses: {persona_summary['length'].idxmax()}\")\n",
    "print(f\"Shortest responses: {persona_summary['length'].idxmin()}\")\n",
    "print(f\"Most questions: {persona_summary['question_count'].idxmax()}\")\n",
    "print(f\"Most complex vocabulary: {persona_summary['avg_word_length'].idxmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../results/exp5_results.json', 'w') as f:\n",
    "    json.dump({\"persona_summary\": persona_summary.to_dict()}, f, indent=2)\n",
    "print(\"Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
