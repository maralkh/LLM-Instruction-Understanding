{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Model Internals - Why Instructions Change Behavior\n",
    "\n",
    "**Goal:** Understand the internal mechanisms through which system prompts affect model outputs.\n",
    "\n",
    "**Analysis Focus:**\n",
    "- Hidden state, attention, MLP, and residual stream changes across layers\n",
    "- Which layers are most affected by system prompts?\n",
    "- Impact of system prompt length, specific tokens/phrases, and token positions\n",
    "- Correlation between internal changes and output changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    import shutil\n",
    "    if os.path.exists('/content/LLM-Instruction-Understanding'):\n",
    "        shutil.rmtree('/content/LLM-Instruction-Understanding')\n",
    "    !git clone https://github.com/maralkh/LLM-Instruction-Understanding.git\n",
    "    os.chdir('/content/LLM-Instruction-Understanding')\n",
    "    !pip install -q -r requirements.txt\n",
    "    sys.path.insert(0, '/content/LLM-Instruction-Understanding')\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from src.model_utils import load_model\n",
    "from src.test_configs import get_all_test_prompts, get_core_system_prompts, get_system_prompts, build_chat_prompt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "layer_info = model.get_layer_info()\n",
    "print(f\"Model: {layer_info['model_name']}\")\n",
    "print(f\"Layers: {layer_info['n_layers']}, Heads: {layer_info['n_heads']}, Hidden: {layer_info['hidden_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Internals Comparison (Hidden States, Attention, MLP, Residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_component_similarities(model, prompt1, prompt2):\n",
    "    \"\"\"\n",
    "    Compute similarities for different model components:\n",
    "    - Hidden states (output of each layer)\n",
    "    - Attention patterns\n",
    "    - MLP outputs (approximated via residual difference)\n",
    "    - Residual stream\n",
    "    \"\"\"\n",
    "    inputs1 = model.tokenizer(prompt1, return_tensors=\"pt\").to(model.config.device)\n",
    "    inputs2 = model.tokenizer(prompt2, return_tensors=\"pt\").to(model.config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out1 = model.model(**inputs1, output_hidden_states=True, output_attentions=True)\n",
    "        out2 = model.model(**inputs2, output_hidden_states=True, output_attentions=True)\n",
    "    \n",
    "    results = {'layer': [], 'component': [], 'cosine_sim': [], 'l2_norm': []}\n",
    "    \n",
    "    n_layers = len(out1.hidden_states) - 1  # Exclude embedding layer\n",
    "    \n",
    "    for layer_idx in range(n_layers):\n",
    "        # Hidden states (layer output)\n",
    "        hs1 = out1.hidden_states[layer_idx + 1][0, -1, :].float().cpu().numpy()\n",
    "        hs2 = out2.hidden_states[layer_idx + 1][0, -1, :].float().cpu().numpy()\n",
    "        \n",
    "        cos_sim = np.dot(hs1, hs2) / (np.linalg.norm(hs1) * np.linalg.norm(hs2) + 1e-10)\n",
    "        l2 = np.linalg.norm(hs1 - hs2)\n",
    "        \n",
    "        results['layer'].append(layer_idx)\n",
    "        results['component'].append('hidden_state')\n",
    "        results['cosine_sim'].append(float(cos_sim) if np.isfinite(cos_sim) else 0.0)\n",
    "        results['l2_norm'].append(float(l2))\n",
    "        \n",
    "        # Attention patterns (average over heads, last query position)\n",
    "        if out1.attentions and layer_idx < len(out1.attentions):\n",
    "            attn1 = out1.attentions[layer_idx][0, :, -1, :].float().cpu().numpy().flatten()\n",
    "            attn2 = out2.attentions[layer_idx][0, :, -1, :].float().cpu().numpy().flatten()\n",
    "            \n",
    "            # Pad to same length\n",
    "            max_len = max(len(attn1), len(attn2))\n",
    "            attn1_pad = np.pad(attn1, (0, max_len - len(attn1)))\n",
    "            attn2_pad = np.pad(attn2, (0, max_len - len(attn2)))\n",
    "            \n",
    "            cos_sim = np.dot(attn1_pad, attn2_pad) / (np.linalg.norm(attn1_pad) * np.linalg.norm(attn2_pad) + 1e-10)\n",
    "            l2 = np.linalg.norm(attn1_pad - attn2_pad)\n",
    "            \n",
    "            results['layer'].append(layer_idx)\n",
    "            results['component'].append('attention')\n",
    "            results['cosine_sim'].append(float(cos_sim) if np.isfinite(cos_sim) else 0.0)\n",
    "            results['l2_norm'].append(float(l2))\n",
    "        \n",
    "        # Residual stream change (difference between consecutive layers)\n",
    "        if layer_idx > 0:\n",
    "            prev_hs1 = out1.hidden_states[layer_idx][0, -1, :].float().cpu().numpy()\n",
    "            prev_hs2 = out2.hidden_states[layer_idx][0, -1, :].float().cpu().numpy()\n",
    "            \n",
    "            # Residual = current - previous (approximates layer contribution)\n",
    "            res1 = hs1 - prev_hs1\n",
    "            res2 = hs2 - prev_hs2\n",
    "            \n",
    "            cos_sim = np.dot(res1, res2) / (np.linalg.norm(res1) * np.linalg.norm(res2) + 1e-10)\n",
    "            l2 = np.linalg.norm(res1 - res2)\n",
    "            \n",
    "            results['layer'].append(layer_idx)\n",
    "            results['component'].append('residual')\n",
    "            results['cosine_sim'].append(float(cos_sim) if np.isfinite(cos_sim) else 0.0)\n",
    "            results['l2_norm'].append(float(l2))\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = get_all_test_prompts()[:5]\n",
    "system_prompts = get_core_system_prompts()\n",
    "baseline_sys = system_prompts['none']\n",
    "\n",
    "all_component_results = []\n",
    "\n",
    "for test in tqdm(test_prompts, desc=\"Analyzing components\"):\n",
    "    for sys_name, sys_info in system_prompts.items():\n",
    "        if sys_name == 'none':\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            prompt_base = build_chat_prompt(baseline_sys['text'], test['prompt'], model.tokenizer)\n",
    "            prompt_var = build_chat_prompt(sys_info['text'], test['prompt'], model.tokenizer)\n",
    "            \n",
    "            comp_df = compute_component_similarities(model, prompt_base, prompt_var)\n",
    "            comp_df['test_id'] = test['id']\n",
    "            comp_df['category'] = test['category']\n",
    "            comp_df['system_prompt'] = sys_name\n",
    "            \n",
    "            all_component_results.append(comp_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {test['id']}/{sys_name}: {e}\")\n",
    "\n",
    "component_df = pd.concat(all_component_results, ignore_index=True)\n",
    "print(f\"Collected {len(component_df)} component measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component similarity by layer\n",
    "component_by_layer = component_df.groupby(['layer', 'component'])['cosine_sim'].mean().unstack()\n",
    "\n",
    "print(\"=== Component Similarity by Layer ===\")\n",
    "print(component_by_layer.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize component similarities across layers\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, component in zip(axes, ['hidden_state', 'attention', 'residual']):\n",
    "    if component in component_by_layer.columns:\n",
    "        data = component_by_layer[component].dropna()\n",
    "        ax.plot(data.index, data.values, 'o-', linewidth=2, markersize=6)\n",
    "        ax.set_xlabel('Layer')\n",
    "        ax.set_ylabel('Cosine Similarity')\n",
    "        ax.set_title(f'{component.replace(\"_\", \" \").title()} Similarity')\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/component_similarity_by_layer.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Component x System Prompt\n",
    "comp_sys = component_df.groupby(['system_prompt', 'component'])['cosine_sim'].mean().unstack()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.heatmap(comp_sys, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax, vmin=0.5, vmax=1.0)\n",
    "ax.set_title('Component Similarity by System Prompt\\n(higher = more similar to baseline)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/component_by_system_prompt.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Prompt Length vs Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all system prompts (not just core) for more length variation\n",
    "all_system_prompts = get_system_prompts()\n",
    "\n",
    "# Calculate system prompt properties\n",
    "sys_properties = []\n",
    "for sys_name, sys_info in all_system_prompts.items():\n",
    "    text = sys_info['text']\n",
    "    tokens = model.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    sys_properties.append({\n",
    "        'system_prompt': sys_name,\n",
    "        'text': text,\n",
    "        'char_length': len(text),\n",
    "        'token_length': tokens.shape[1],\n",
    "        'word_count': len(text.split()),\n",
    "        'has_persona': 'you are' in text.lower() or 'act as' in text.lower(),\n",
    "        'has_format': 'format' in text.lower() or 'structure' in text.lower(),\n",
    "        'has_thinking': 'step' in text.lower() or 'think' in text.lower() or 'reason' in text.lower(),\n",
    "        'has_constraint': 'must' in text.lower() or 'always' in text.lower() or 'never' in text.lower(),\n",
    "        'has_concise': 'concise' in text.lower() or 'brief' in text.lower() or 'short' in text.lower(),\n",
    "    })\n",
    "\n",
    "sys_props_df = pd.DataFrame(sys_properties)\n",
    "print(\"=== System Prompt Properties ===\")\n",
    "print(sys_props_df[['system_prompt', 'token_length', 'word_count', 'has_persona', 'has_thinking', 'has_constraint']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect impact data for all system prompts\n",
    "test_subset = get_all_test_prompts()[:3]  # Use fewer tests for speed\n",
    "baseline_sys = all_system_prompts.get('none', {'text': ''})\n",
    "\n",
    "length_impact_results = []\n",
    "\n",
    "for test in tqdm(test_subset, desc=\"Length analysis\"):\n",
    "    for sys_name, sys_info in all_system_prompts.items():\n",
    "        if sys_name == 'none':\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            prompt_base = build_chat_prompt(baseline_sys['text'], test['prompt'], model.tokenizer)\n",
    "            prompt_var = build_chat_prompt(sys_info['text'], test['prompt'], model.tokenizer)\n",
    "            \n",
    "            comparison = model.compare_internals(prompt_base, prompt_var)\n",
    "            \n",
    "            # Get average hidden state similarity across layers\n",
    "            avg_hs_sim = np.mean([v['cosine_sim'] for v in comparison['hidden_state_diff'].values()])\n",
    "            \n",
    "            # Get properties\n",
    "            props = sys_props_df[sys_props_df['system_prompt'] == sys_name].iloc[0]\n",
    "            \n",
    "            length_impact_results.append({\n",
    "                'test_id': test['id'],\n",
    "                'system_prompt': sys_name,\n",
    "                'token_length': props['token_length'],\n",
    "                'word_count': props['word_count'],\n",
    "                'has_persona': props['has_persona'],\n",
    "                'has_thinking': props['has_thinking'],\n",
    "                'has_constraint': props['has_constraint'],\n",
    "                'has_concise': props['has_concise'],\n",
    "                'avg_hs_similarity': avg_hs_sim,\n",
    "                'logit_similarity': comparison['logit_diff']['cosine_sim'],\n",
    "                'top_token_same': comparison['logit_diff']['top_token_same']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "length_df = pd.DataFrame(length_impact_results)\n",
    "print(f\"Collected {len(length_df)} length impact measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation: Length vs Impact\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Token length vs hidden state similarity\n",
    "ax = axes[0]\n",
    "grouped = length_df.groupby('system_prompt').agg({\n",
    "    'token_length': 'first',\n",
    "    'avg_hs_similarity': 'mean'\n",
    "}).reset_index()\n",
    "ax.scatter(grouped['token_length'], grouped['avg_hs_similarity'], s=100, alpha=0.7)\n",
    "for _, row in grouped.iterrows():\n",
    "    ax.annotate(row['system_prompt'], (row['token_length'], row['avg_hs_similarity']), fontsize=8)\n",
    "\n",
    "# Add correlation line\n",
    "if len(grouped) > 2:\n",
    "    z = np.polyfit(grouped['token_length'], grouped['avg_hs_similarity'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(grouped['token_length'], p(grouped['token_length']), 'r--', alpha=0.5)\n",
    "    corr, pval = stats.pearsonr(grouped['token_length'], grouped['avg_hs_similarity'])\n",
    "else:\n",
    "    corr, pval = 0, 1\n",
    "ax.set_xlabel('System Prompt Token Length')\n",
    "ax.set_ylabel('Avg Hidden State Similarity')\n",
    "ax.set_title(f'Length vs HS Similarity\\nr={corr:.3f}, p={pval:.3f}')\n",
    "\n",
    "# Token length vs logit similarity\n",
    "ax = axes[1]\n",
    "grouped2 = length_df.groupby('system_prompt').agg({\n",
    "    'token_length': 'first',\n",
    "    'logit_similarity': 'mean'\n",
    "}).reset_index()\n",
    "ax.scatter(grouped2['token_length'], grouped2['logit_similarity'], s=100, alpha=0.7, color='orange')\n",
    "if len(grouped2) > 2:\n",
    "    corr2, pval2 = stats.pearsonr(grouped2['token_length'], grouped2['logit_similarity'])\n",
    "else:\n",
    "    corr2, pval2 = 0, 1\n",
    "ax.set_xlabel('System Prompt Token Length')\n",
    "ax.set_ylabel('Logit Similarity')\n",
    "ax.set_title(f'Length vs Logit Similarity\\nr={corr2:.3f}, p={pval2:.3f}')\n",
    "\n",
    "# Token length vs top token change rate\n",
    "ax = axes[2]\n",
    "grouped3 = length_df.groupby('system_prompt').agg({\n",
    "    'token_length': 'first',\n",
    "    'top_token_same': 'mean'\n",
    "}).reset_index()\n",
    "ax.scatter(grouped3['token_length'], 1 - grouped3['top_token_same'], s=100, alpha=0.7, color='green')\n",
    "ax.set_xlabel('System Prompt Token Length')\n",
    "ax.set_ylabel('Top Token Change Rate')\n",
    "ax.set_title('Length vs Output Change Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/length_vs_impact.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Token/Phrase Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze impact of specific phrases\n",
    "phrase_features = ['has_persona', 'has_thinking', 'has_constraint', 'has_concise']\n",
    "\n",
    "phrase_impact = []\n",
    "for feature in phrase_features:\n",
    "    with_feature = length_df[length_df[feature] == True]['avg_hs_similarity'].mean()\n",
    "    without_feature = length_df[length_df[feature] == False]['avg_hs_similarity'].mean()\n",
    "    \n",
    "    # Handle NaN\n",
    "    if np.isnan(with_feature):\n",
    "        with_feature = 0.0\n",
    "    if np.isnan(without_feature):\n",
    "        without_feature = 0.0\n",
    "    \n",
    "    phrase_impact.append({\n",
    "        'feature': feature.replace('has_', ''),\n",
    "        'with_feature': with_feature,\n",
    "        'without_feature': without_feature,\n",
    "        'impact': without_feature - with_feature  # Lower similarity = more impact\n",
    "    })\n",
    "\n",
    "phrase_df = pd.DataFrame(phrase_impact).sort_values('impact', ascending=False)\n",
    "print(\"=== Phrase Impact on Hidden State Similarity ===\")\n",
    "print(\"(Higher impact = phrase causes more deviation from baseline)\")\n",
    "print(phrase_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize phrase impact\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(phrase_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, phrase_df['with_feature'], width, label='With phrase', color='coral')\n",
    "bars2 = ax.bar(x + width/2, phrase_df['without_feature'], width, label='Without phrase', color='steelblue')\n",
    "\n",
    "ax.set_ylabel('Avg Hidden State Similarity to Baseline')\n",
    "ax.set_title('Impact of Specific Phrases in System Prompts')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(phrase_df['feature'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/phrase_impact.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Token Position Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_to_positions(model, prompt, system_prompt_length):\n",
    "    \"\"\"\n",
    "    Analyze where the model attends based on token positions.\n",
    "    Returns attention to: system prompt region, user prompt region, recent tokens.\n",
    "    \"\"\"\n",
    "    inputs = model.tokenizer(prompt, return_tensors=\"pt\").to(model.config.device)\n",
    "    seq_len = inputs.input_ids.shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.model(**inputs, output_attentions=True)\n",
    "    \n",
    "    results = []\n",
    "    for layer_idx, attn in enumerate(outputs.attentions):\n",
    "        # attn shape: (batch, heads, seq_len, seq_len)\n",
    "        # Look at last token's attention pattern, averaged over heads\n",
    "        last_token_attn = attn[0, :, -1, :].mean(dim=0).float().cpu().numpy()\n",
    "        \n",
    "        # Define regions\n",
    "        sys_end = min(system_prompt_length, seq_len)\n",
    "        user_start = system_prompt_length\n",
    "        user_end = max(user_start, seq_len - 5)\n",
    "        recent_start = max(0, seq_len - 5)\n",
    "        \n",
    "        attn_sys = float(last_token_attn[:sys_end].sum()) if sys_end > 0 else 0.0\n",
    "        attn_user = float(last_token_attn[user_start:user_end].sum()) if user_end > user_start else 0.0\n",
    "        attn_recent = float(last_token_attn[recent_start:].sum())\n",
    "        \n",
    "        # Entropy\n",
    "        probs = np.clip(last_token_attn, 1e-10, 1.0)\n",
    "        entropy = float(-np.sum(probs * np.log(probs)))\n",
    "        \n",
    "        results.append({\n",
    "            'layer': layer_idx,\n",
    "            'attn_to_system': attn_sys,\n",
    "            'attn_to_user': attn_user,\n",
    "            'attn_to_recent': attn_recent,\n",
    "            'attn_entropy': entropy if np.isfinite(entropy) else 0.0\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention to different positions for different system prompts\n",
    "test_prompt = test_prompts[0]\n",
    "\n",
    "position_results = []\n",
    "for sys_name, sys_info in list(all_system_prompts.items())[:8]:  # Limit for speed\n",
    "    full_prompt = build_chat_prompt(sys_info['text'], test_prompt['prompt'], model.tokenizer)\n",
    "    sys_tokens = model.tokenizer(sys_info['text'], return_tensors=\"pt\").input_ids.shape[1] if sys_info['text'] else 0\n",
    "    \n",
    "    pos_df = analyze_attention_to_positions(model, full_prompt, sys_tokens)\n",
    "    pos_df['system_prompt'] = sys_name\n",
    "    pos_df['sys_token_length'] = sys_tokens\n",
    "    position_results.append(pos_df)\n",
    "\n",
    "position_df = pd.concat(position_results, ignore_index=True)\n",
    "print(f\"Collected {len(position_df)} position measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention to different regions by layer\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Aggregate by layer\n",
    "layer_pos = position_df.groupby('layer').agg({\n",
    "    'attn_to_system': 'mean',\n",
    "    'attn_to_user': 'mean',\n",
    "    'attn_to_recent': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.plot(layer_pos['layer'], layer_pos['attn_to_system'], 'o-', label='System Prompt', linewidth=2)\n",
    "ax.plot(layer_pos['layer'], layer_pos['attn_to_user'], 's-', label='User Prompt', linewidth=2)\n",
    "ax.plot(layer_pos['layer'], layer_pos['attn_to_recent'], '^-', label='Recent Tokens', linewidth=2)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Attention Weight')\n",
    "ax.set_title('Attention to Different Regions by Layer')\n",
    "ax.legend()\n",
    "\n",
    "# Heatmap: System prompt x attention to system region by layer\n",
    "ax = axes[0, 1]\n",
    "pivot = position_df.pivot_table(values='attn_to_system', index='system_prompt', columns='layer', aggfunc='mean')\n",
    "sns.heatmap(pivot, cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Attention to System'})\n",
    "ax.set_title('Attention to System Prompt Region')\n",
    "\n",
    "# Correlation: System prompt length vs attention to system\n",
    "ax = axes[1, 0]\n",
    "sys_attn = position_df.groupby('system_prompt').agg({\n",
    "    'sys_token_length': 'first',\n",
    "    'attn_to_system': 'mean'\n",
    "}).reset_index()\n",
    "ax.scatter(sys_attn['sys_token_length'], sys_attn['attn_to_system'], s=100)\n",
    "for _, row in sys_attn.iterrows():\n",
    "    ax.annotate(row['system_prompt'], (row['sys_token_length'], row['attn_to_system']), fontsize=8)\n",
    "ax.set_xlabel('System Prompt Token Length')\n",
    "ax.set_ylabel('Mean Attention to System Region')\n",
    "ax.set_title('Longer System Prompts -> More Attention?')\n",
    "\n",
    "# Attention entropy by system prompt\n",
    "ax = axes[1, 1]\n",
    "entropy_by_sys = position_df.groupby('system_prompt')['attn_entropy'].mean().sort_values()\n",
    "ax.barh(range(len(entropy_by_sys)), entropy_by_sys.values, color='teal', alpha=0.7)\n",
    "ax.set_yticks(range(len(entropy_by_sys)))\n",
    "ax.set_yticklabels(entropy_by_sys.index)\n",
    "ax.set_xlabel('Mean Attention Entropy')\n",
    "ax.set_title('Attention Focus by System Prompt\\n(Lower = More Focused)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/position_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL INTERNALS ANALYSIS - KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. COMPONENT SIMILARITY (averaged across system prompts):\")\n",
    "comp_avg = component_df.groupby('component')['cosine_sim'].mean().sort_values()\n",
    "for comp, sim in comp_avg.items():\n",
    "    print(f\"   - {comp}: {sim:.4f} similarity to baseline\")\n",
    "\n",
    "print(\"\\n2. LENGTH CORRELATION:\")\n",
    "print(f\"   - Token length vs HS similarity: r={corr:.3f} (p={pval:.3f})\")\n",
    "interpretation = 'Longer prompts cause MORE change' if corr < 0 else 'Length has minimal effect'\n",
    "print(f\"   - Interpretation: {interpretation}\")\n",
    "\n",
    "print(\"\\n3. PHRASE IMPACT (which phrases cause most deviation):\")\n",
    "if len(phrase_df) > 0:\n",
    "    top_phrase = phrase_df.iloc[0]\n",
    "    print(f\"   - Most impactful: '{top_phrase['feature']}' (impact={top_phrase['impact']:.4f})\")\n",
    "\n",
    "print(\"\\n4. POSITION ATTENTION:\")\n",
    "layer_mid = layer_info['n_layers'] // 2\n",
    "mid_data = layer_pos[layer_pos['layer'] == layer_mid]\n",
    "if len(mid_data) > 0:\n",
    "    mid_layer_data = mid_data.iloc[0]\n",
    "    print(f\"   - Middle layer ({layer_mid}) attention distribution:\")\n",
    "    print(f\"     - To system prompt: {mid_layer_data['attn_to_system']:.3f}\")\n",
    "    print(f\"     - To user prompt: {mid_layer_data['attn_to_user']:.3f}\")\n",
    "    print(f\"     - To recent tokens: {mid_layer_data['attn_to_recent']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "import json\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "results = {\n",
    "    'component_by_layer': component_by_layer.to_dict(),\n",
    "    'phrase_impact': phrase_df.to_dict('records'),\n",
    "    'length_correlation': {'r': float(corr), 'p': float(pval)},\n",
    "    'model_info': layer_info\n",
    "}\n",
    "\n",
    "with open('../results/internals_full_analysis.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=float)\n",
    "\n",
    "# Save dataframes\n",
    "component_df.to_csv('../results/component_analysis.csv', index=False)\n",
    "length_df.to_csv('../results/length_analysis.csv', index=False)\n",
    "position_df.to_csv('../results/position_analysis.csv', index=False)\n",
    "\n",
    "print(\"All results saved to ../results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
